# ЁЯУЪ Playwright рж╢рзЗржЦрж╛рж░ ржЧрж╛ржЗржб - ржжрзНрж░рзБржд рж╢рзБрж░рзБ

## ЁЯОп ржПржЗ ржЧрж╛ржЗржбрзЗ ржХрж┐ ржЖржЫрзЗ?

ржПржЯрж┐ ржПржХржЯрж┐ **ржжрзНрж░рзБржд рж╢рзЗржЦрж╛рж░ ржЧрж╛ржЗржб** ржпрзЗржЦрж╛ржирзЗ ржЖржкржирж┐ рзз-рзи ржШржирзНржЯрж╛ржпрж╝ Playwright ржПрж░ ржорзВрж▓ ржмрж┐рж╖ржпрж╝ржЧрзБрж▓рзЛ рж╢рж┐ржЦрждрзЗ ржкрж╛рж░ржмрзЗржиред

### ЁЯЪА **ржХрж┐ рж╢рж┐ржЦржмрзЗржи:**
- тЬЕ Playwright ржХрж┐ ржПржмржВ ржХрзЗржи ржмрзНржпржмрж╣рж╛рж░ ржХрж░ржмрзЗржи
- тЬЕ Installation ржУ setup
- тЬЕ ржкрзНрж░ржержо scraping script
- тЬЕ Element selection techniques
- тЬЕ Data extraction methods
- тЬЕ Common problems ржУ solutions

---

## ЁЯдФ Playwright ржХрж┐?

### ЁЯОн **рж╕рж╣ржЬ ржнрж╛рж╖рж╛ржпрж╝:**
Playwright рж╣рж▓рзЛ ржПржХржЯрж┐ **browser automation tool** ржпрж╛ ржжрж┐ржпрж╝рзЗ ржЖржкржирж┐:
- ЁЯМР ржпрзЗржХрзЛржирзЛ website automatically visit ржХрж░рждрзЗ ржкрж╛рж░рзЗржи
- ЁЯЦ▒я╕П Button click, form fill ржХрж░рждрзЗ ржкрж╛рж░рзЗржи
- ЁЯУК Data extract ржХрж░рждрзЗ ржкрж╛рж░рзЗржи
- ЁЯУ╕ Screenshot ржирж┐рждрзЗ ржкрж╛рж░рзЗржи
- ЁЯУД PDF generate ржХрж░рждрзЗ ржкрж╛рж░рзЗржи

### ЁЯЖЪ **ржЕржирзНржпрж╛ржирзНржп tools ржПрж░ рждрзБрж▓ржирж╛:**

| Feature | Playwright | Selenium | Requests+BS4 |
|---------|------------|----------|--------------|
| **Speed** | тЪб ржЦрзБржм ржжрзНрж░рзБржд | ЁЯРМ ржзрзАрж░ | тЪб ржжрзНрж░рзБржд |
| **JavaScript** | тЬЕ рж╕ржорзНржкрзВрж░рзНржг support | тЬЕ рж╕ржорзНржкрзВрж░рзНржг support | тЭМ No support |
| **Multiple browsers** | тЬЕ Chrome, Firefox, Safari | тЬЕ Chrome, Firefox | тЭМ No browser |
| **Mobile emulation** | тЬЕ Perfect | тЪая╕П Limited | тЭМ No support |
| **Setup difficulty** | ЁЯЯв рж╕рж╣ржЬ | ЁЯЯб ржоржзрзНржпржо | ЁЯЯв рж╕рж╣ржЬ |

### ЁЯОп **ржХржЦржи Playwright ржмрзНржпржмрж╣рж╛рж░ ржХрж░ржмрзЗржи:**
- тЬЕ JavaScript heavy websites
- тЬЕ SPA (Single Page Applications)
- тЬЕ Login required sites
- тЬЕ Dynamic content loading
- тЬЕ Mobile responsive testing

---

## ЁЯФз Installation ржУ Setup

### ЁЯУж **Step 1: Python Environment**
```bash
# Virtual environment рждрзИрж░рж┐ ржХрж░рзБржи
python -m venv playwright-env

# Activate ржХрж░рзБржи
# Windows:
playwright-env\Scripts\activate
# Linux/Mac:
source playwright-env/bin/activate
```

### ЁЯУе **Step 2: Playwright Install**
```bash
# Playwright install ржХрж░рзБржи
pip install playwright

# Browsers download ржХрж░рзБржи (ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг!)
playwright install

# ржЕржержмрж╛ specific browser
playwright install chromium
```

### тЬЕ **Step 3: Test Installation**
```python
# test.py
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch(headless=False)
    page = browser.new_page()
    page.goto("https://google.com")
    print("тЬЕ Playwright ржХрж╛ржЬ ржХрж░ржЫрзЗ!")
    browser.close()
```

```bash
python test.py
```

---

## ЁЯЪА ржкрзНрж░ржержо Scraping Script

### ЁЯОп **рж▓ржХрзНрж╖рзНржп:** Quotes website ржерзЗржХрзЗ quotes collect ржХрж░рж╛

#### **Step 1: Basic Structure**
```python
# quotes_scraper.py
from playwright.sync_api import sync_playwright

def scrape_quotes():
    """Quotes scrape ржХрж░рж╛рж░ function"""
    
    with sync_playwright() as p:
        # Browser launch ржХрж░рзБржи
        browser = p.chromium.launch(headless=False)  # headless=True production ржП
        
        # ржирждрзБржи page ржЦрзБрж▓рзБржи
        page = browser.new_page()
        
        # Website ржП ржпрж╛ржи
        page.goto("https://quotes.toscrape.com")
        
        # Page load рж╣ржУржпрж╝рж╛рж░ ржЬржирзНржп ржЕржкрзЗржХрзНрж╖рж╛
        page.wait_for_load_state('networkidle')
        
        print("тЬЕ Website loaded successfully!")
        
        # Browser ржмржирзНржз ржХрж░рзБржи
        browser.close()

# Function call ржХрж░рзБржи
scrape_quotes()
```

#### **Step 2: Data Extract ржХрж░рж╛**
```python
def scrape_quotes():
    """Quotes extract ржХрж░рж╛"""
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()
        page.goto("https://quotes.toscrape.com")
        
        # рж╕ржм quotes ржЦрзБржБржЬрзБржи
        quotes = page.locator(".quote").all()
        
        print(f"ЁЯУЪ ржорзЛржЯ quotes ржкрж╛ржУржпрж╝рж╛ ржЧрзЗржЫрзЗ: {len(quotes)}")
        
        # ржкрзНрж░рждрж┐ржЯрж┐ quote process ржХрж░рзБржи
        for i, quote in enumerate(quotes, 1):
            # Text extract ржХрж░рзБржи
            text = quote.locator(".text").text_content()
            
            # Author extract ржХрж░рзБржи
            author = quote.locator(".author").text_content()
            
            # Tags extract ржХрж░рзБржи
            tags = quote.locator(".tag").all_text_contents()
            
            print(f"\n--- Quote {i} ---")
            print(f"ЁЯУЭ Text: {text}")
            print(f"ЁЯСд Author: {author}")
            print(f"ЁЯП╖я╕П Tags: {', '.join(tags)}")
        
        browser.close()

scrape_quotes()
```

#### **Step 3: Data Save ржХрж░рж╛**
```python
import json
from datetime import datetime

def scrape_quotes():
    """Quotes scrape ржХрж░рзЗ file ржП save ржХрж░рж╛"""
    
    quotes_data = []  # Data store ржХрж░рж╛рж░ ржЬржирзНржп list
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()
        page.goto("https://quotes.toscrape.com")
        
        quotes = page.locator(".quote").all()
        
        for quote in quotes:
            quote_info = {
                'text': quote.locator(".text").text_content(),
                'author': quote.locator(".author").text_content(),
                'tags': quote.locator(".tag").all_text_contents(),
                'scraped_at': datetime.now().isoformat()
            }
            
            quotes_data.append(quote_info)
        
        browser.close()
    
    # JSON file ржП save ржХрж░рзБржи
    with open('quotes.json', 'w', encoding='utf-8') as f:
        json.dump(quotes_data, f, ensure_ascii=False, indent=2)
    
    print(f"тЬЕ {len(quotes_data)}ржЯрж┐ quotes save рж╣ржпрж╝рзЗржЫрзЗ quotes.json file ржП")

scrape_quotes()
```

---

## ЁЯОп Element Selection Techniques

### ЁЯФН **ржмрж┐ржнрж┐ржирзНржи ржЙржкрж╛ржпрж╝рзЗ element ржЦрзБржБржЬрзЗ ржмрзЗрж░ ржХрж░рж╛:**

#### **1. CSS Selectors:**
```python
# Class ржжрж┐ржпрж╝рзЗ
page.locator(".quote")           # class="quote"
page.locator("#main")            # id="main"

# Tag ржжрж┐ржпрж╝рзЗ
page.locator("h1")               # <h1> tag
page.locator("div")              # <div> tag

# Attribute ржжрж┐ржпрж╝рзЗ
page.locator("[data-id='123']")  # data-id="123"
page.locator("input[type='text']") # input type="text"

# Combination
page.locator("div.quote p.text") # div class="quote" ржПрж░ ржнрж┐рждрж░ p class="text"
```

#### **2. Text Content ржжрж┐ржпрж╝рзЗ:**
```python
# Exact text match
page.locator("text=Click me")

# Partial text match
page.locator("text=Click")

# Case insensitive
page.locator("text=click me >> i")
```

#### **3. XPath (Advanced):**
```python
# XPath selector
page.locator("xpath=//div[@class='quote']")
page.locator("xpath=//a[contains(text(), 'Next')]")
```

### ЁЯУК **Data Extract ржХрж░рж╛рж░ methods:**

#### **Text Content:**
```python
element = page.locator(".author")

# Single element
text = element.text_content()

# Multiple elements
texts = element.all_text_contents()
```

#### **Attributes:**
```python
# href attribute
link = page.locator("a").get_attribute("href")

# src attribute
image = page.locator("img").get_attribute("src")

# All attributes
attrs = page.locator("div").evaluate("el => el.attributes")
```

#### **HTML Content:**
```python
# Inner HTML
html = page.locator(".quote").inner_html()

# Outer HTML
full_html = page.locator(".quote").outer_html()
```

---

## ЁЯФД Multiple Pages Handle ржХрж░рж╛

### ЁЯУД **Pagination Handle:**
```python
def scrape_all_pages():
    """рж╕ржм pages ржерзЗржХрзЗ quotes scrape ржХрж░рж╛"""
    
    all_quotes = []
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()
        page.goto("https://quotes.toscrape.com")
        
        page_number = 1
        
        while True:
            print(f"ЁЯУД Scraping page {page_number}...")
            
            # Current page ржПрж░ quotes extract ржХрж░рзБржи
            quotes = page.locator(".quote").all()
            
            for quote in quotes:
                quote_data = {
                    'text': quote.locator(".text").text_content(),
                    'author': quote.locator(".author").text_content(),
                    'page': page_number
                }
                all_quotes.append(quote_data)
            
            # Next button ржЖржЫрзЗ ржХрж┐ржирж╛ check ржХрж░рзБржи
            next_button = page.locator(".next a")
            
            if next_button.count() == 0:
                print("тЬЕ рж╕ржм pages complete!")
                break
            
            # Next page ржП ржпрж╛ржи
            next_button.click()
            page.wait_for_load_state('networkidle')
            page_number += 1
        
        browser.close()
    
    print(f"ЁЯУК ржорзЛржЯ {len(all_quotes)}ржЯрж┐ quotes collect ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗ")
    return all_quotes

# Function call
all_quotes = scrape_all_pages()
```

---

## тЪая╕П Common Problems ржУ Solutions

### тЭМ **Problem 1: Element not found**
```python
# тЭМ Wrong way
element = page.locator(".not-exist").text_content()  # Error!

# тЬЕ Right way
element = page.locator(".not-exist")
if element.count() > 0:
    text = element.text_content()
else:
    print("Element ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржпрж╝ржирж┐")
```

### тЭМ **Problem 2: Page not loaded**
```python
# тЭМ Wrong way
page.goto("https://example.com")
page.locator(".content").click()  # Element may not be ready

# тЬЕ Right way
page.goto("https://example.com")
page.wait_for_load_state('networkidle')  # Wait for page to load
page.locator(".content").click()
```

### тЭМ **Problem 3: Dynamic content**
```python
# тЭМ Wrong way
page.goto("https://example.com")
data = page.locator(".dynamic-content").text_content()  # May be empty

# тЬЕ Right way
page.goto("https://example.com")
page.wait_for_selector(".dynamic-content", timeout=10000)  # Wait for element
data = page.locator(".dynamic-content").text_content()
```

### тЭМ **Problem 4: Browser detection**
```python
# тЬЕ Human-like browser setup
browser = p.chromium.launch(
    headless=False,
    args=[
        '--disable-blink-features=AutomationControlled',
        '--disable-dev-shm-usage',
        '--no-sandbox'
    ]
)

context = browser.new_context(
    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    viewport={'width': 1366, 'height': 768}
)

page = context.new_page()
```

---

## ЁЯОп Next Steps

### ЁЯУЪ **ржЖрж░рзЛ рж╢рж┐ржЦрждрзЗ ржЪрж╛ржи?**

#### **ЁЯФ░ Beginner Level:**
1. **[рж╕ржорзНржкрзВрж░рзНржг ржЧрж╛ржЗржб](./рж╕ржорзНржкрзВрж░рзНржг-playwright-ржЧрж╛ржЗржб.md)** - ржмрж┐рж╕рзНрждрж╛рж░рж┐ржд tutorial
2. **[ржоржбрж┐ржЙрж▓ ржЧрж╛ржЗржб](./ржоржбрж┐ржЙрж▓-ржЗржирж╕рзНржЯрж▓рзЗрж╢ржи-ржЧрж╛ржЗржб.md)** - Module installation

#### **ЁЯЪА Intermediate Level:**
1. **[ржЙржирзНржиржд ржХрзМрж╢рж▓](./ржЙржирзНржиржд-рж╕рзНржХрзНрж░рзНржпрж╛ржкрж┐ржВ-ржХрзМрж╢рж▓.md)** - Advanced techniques
2. **[ржХрзБржХрж┐ ржмрзНржпржмрж╕рзНржерж╛ржкржирж╛](./ржХрзБржХрж┐-рж╕рзЗрж╢ржи-ржмрзНржпржмрж╕рзНржерж╛ржкржирж╛.md)** - Login handling

#### **ЁЯОп Advanced Level:**
1. **[ржмрж╛рж╕рзНрждржм ржкрзНрж░ржЬрзЗржХрзНржЯ](./ржмрж╛рж╕рзНрждржм-ржкрзНрж░ржЬрзЗржХрзНржЯ-ржЙржжрж╛рж╣рж░ржг.md)** - Real-world examples
2. **[ржбрзЗржЯрж╛ ржкрзНрж░рж╕рзЗрж╕рж┐ржВ](./ржбрзЗржЯрж╛-ржкрзНрж░рж╕рзЗрж╕рж┐ржВ-ржмрж┐рж╢рзНрж▓рзЗрж╖ржг.md)** - Data analysis

### ЁЯЫая╕П **Practice Projects:**
1. **News Scraper** - ржЦржмрж░рзЗрж░ website ржерзЗржХрзЗ headlines collect
2. **Product Price Tracker** - e-commerce site ржерзЗржХрзЗ price monitor
3. **Job Portal Scraper** - job listings collect
4. **Social Media Monitor** - posts ржУ comments track

---

## ЁЯОЙ рж╕ржорж╛ржкржирзА

ржПржЗ ржЧрж╛ржЗржб ржжрж┐ржпрж╝рзЗ ржЖржкржирж┐ Playwright ржПрж░ **ржорзВрж▓ ржмрж┐рж╖ржпрж╝ржЧрзБрж▓рзЛ** рж╢рж┐ржЦрзЗржЫрзЗржи:

тЬЕ **Installation ржУ setup**  
тЬЕ **Basic scraping techniques**  
тЬЕ **Element selection methods**  
тЬЕ **Data extraction ржУ saving**  
тЬЕ **Common problems ржУ solutions**  

### ЁЯЪА **ржПржЦржи ржЖржкржирж┐ ржкрж╛рж░ржмрзЗржи:**
- Simple websites scrape ржХрж░рждрзЗ
- Data extract ржХрж░рзЗ file ржП save ржХрж░рждрзЗ
- Multiple pages handle ржХрж░рждрзЗ
- Common errors solve ржХрж░рждрзЗ

**ржкрж░ржмрж░рзНрждрзА step: [рж╕ржорзНржкрзВрж░рзНржг ржЧрж╛ржЗржб](./рж╕ржорзНржкрзВрж░рзНржг-playwright-ржЧрж╛ржЗржб.md) ржкржбрж╝рзБржи advanced techniques рж╢рзЗржЦрж╛рж░ ржЬржирзНржп!**

**рж╢рзБржн рж╕рзНржХрзНрж░рзНржпрж╛ржкрж┐ржВ! ЁЯХ╖я╕ПЁЯЗзЁЯЗй**
