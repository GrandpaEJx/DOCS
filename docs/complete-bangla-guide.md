# ЁЯОн Playwright ржжрж┐ржпрж╝рзЗ ржУржпрж╝рзЗржм рж╕рзНржХрзНрж░рзНржпрж╛ржкрж┐ржВ - A to Z рж╕ржорзНржкрзВрж░рзНржг ржЧрж╛ржЗржб ЁЯЗзЁЯЗй

## ЁЯУЪ рж╕рзВржЪрж┐ржкрждрзНрж░
1. [Playwright ржХрж┐ ржПржмржВ ржХрзЗржи?](#what-is-playwright)
2. [ржЗржирж╕рзНржЯрж▓рзЗрж╢ржи ржУ рж╕рзЗржЯржЖржк](#installation-setup)
3. [ржбрзЗржнрзЗрж▓ржкрж╛рж░ ржЯрзБрж▓рж╕ (DevTools) ржорж╛рж╕рзНржЯрж╛рж░рж┐](#devtools-mastery)
4. [ржирзЗржЯржУржпрж╝рж╛рж░рзНржХ ржЯрзНржпрж╛ржм ржоржирж┐ржЯрж░рж┐ржВ](#network-monitoring)
5. [HTML/CSS рж╕рж┐рж▓рзЗржХрзНржЯрж░ ржПржХрзНрж╕ржкрж╛рж░рзНржЯ](#selectors-expert)
6. [Playwright ржмрзЗрж╕рж┐ржХ - Browser ржУ Page](#playwright-basics)
7. [Element Interaction - Click, Type, Fill](#element-interaction)
8. [ржЕрзНржпрж╛ржбржнрж╛ржирзНрж╕ рж╕рзНржХрзНрж░рзНржпрж╛ржкрж┐ржВ ржЯрзЗржХржирж┐ржХ](#advanced-scraping)
9. [JavaScript Execution ржУ Evaluation](#javascript-execution)
10. [Request/Response Interception](#request-interception)
11. [Mobile ржУ Multi-Browser Testing](#mobile-multi-browser)
12. [Performance ржУ Optimization](#performance-optimization)
13. [Error Handling ржУ Debugging](#error-handling)
14. [Real-World ржкрзНрж░ржЬрзЗржХрзНржЯ](#real-world-projects)
15. [Best Practices ржУ Security](#best-practices)

---

## ЁЯОн Playwright ржХрж┐ ржПржмржВ ржХрзЗржи? {#what-is-playwright}

### ЁЯдФ Playwright ржХрж┐?
**Playwright** рж╣рж▓рзЛ Microsoft ржПрж░ рждрзИрж░рж┐ ржПржХржЯрж┐ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА browser automation library ржпрж╛ ржжрж┐ржпрж╝рзЗ ржЖржкржирж┐:
- **Real Browser Control:** Chrome, Firefox, Safari ржирж┐ржпрж╝ржирзНрждрзНрж░ржг ржХрж░рждрзЗ ржкрж╛рж░рзЗржи
- **JavaScript Rendering:** Dynamic content load ржХрж░рждрзЗ ржкрж╛рж░рзЗржи
- **User Interaction:** Click, type, scroll ржХрж░рждрзЗ ржкрж╛рж░рзЗржи
- **Mobile Testing:** Mobile device emulate ржХрж░рждрзЗ ржкрж╛рж░рзЗржи

### ЁЯЪА ржХрзЗржи Playwright ржмрзНржпржмрж╣рж╛рж░ ржХрж░ржмрзЗржи?

#### тЬЕ **Playwright ржПрж░ рж╕рзБржмрж┐ржзрж╛:**
- **Fast & Reliable:** ржЕржирзНржпрж╛ржирзНржп tools ржерзЗржХрзЗ ржжрзНрж░рзБржд
- **Multi-Browser:** Chrome, Firefox, Safari support
- **Mobile Support:** iOS/Android emulation
- **Network Control:** Request/Response intercept ржХрж░рждрзЗ ржкрж╛рж░рзЗржи
- **Screenshot/PDF:** Page capture ржХрж░рждрзЗ ржкрж╛рж░рзЗржи
- **Headless/Headed:** ржжрзБржЯрзЛ mode ржП ржХрж╛ржЬ ржХрж░рзЗ

#### тЭМ **ржЕржирзНржпрж╛ржирзНржп Tools ржПрж░ рж╕ржорж╕рзНржпрж╛:**
- **Requests + BeautifulSoup:** JavaScript render ржХрж░рзЗ ржирж╛
- **Selenium:** ржзрзАрж░ ржПржмржВ unstable
- **Puppeteer:** рж╢рзБржзрзБ Chrome support

### ЁЯОп ржХржЦржи Playwright ржмрзНржпржмрж╣рж╛рж░ ржХрж░ржмрзЗржи?
- тЬЕ **Dynamic Content:** JavaScript ржжрж┐ржпрж╝рзЗ load рж╣ржУржпрж╝рж╛ content
- тЬЕ **User Interaction:** Form fill, button click ржкрзНрж░ржпрж╝рзЛржЬржи
- тЬЕ **SPA (Single Page Apps):** React, Vue, Angular sites
- тЬЕ **Authentication:** Login ржХрж░рзЗ data ржирж┐рждрзЗ рж╣ржмрзЗ
- тЬЕ **Complex Navigation:** Multi-step process

---

## ЁЯЫая╕П ржЗржирж╕рзНржЯрж▓рзЗрж╢ржи ржУ рж╕рзЗржЯржЖржк {#installation-setup}

### ЁЯУЛ ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝ рж╕ржлржЯржУржпрж╝рзНржпрж╛рж░:
1. **Python 3.8+** (ржЕржмрж╢рзНржпржЗ ржкрзНрж░ржпрж╝рзЛржЬржи)
2. **VS Code** ржмрж╛ ржпрзЗржХрзЛржирзЛ code editor
3. **Terminal/Command Prompt**

### ЁЯРН Playwright ржЗржирж╕рзНржЯрж▓ ржХрж░рзБржи:
```bash
# Playwright Python package install
pip install playwright

# Browser binaries install (ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг!)
playwright install

# Specific browser install ржХрж░рждрзЗ ржЪрж╛ржЗрж▓рзЗ
playwright install chromium
playwright install firefox
playwright install webkit
```

### тЬЕ ржЗржирж╕рзНржЯрж▓рзЗрж╢ржи ржЯрзЗрж╕рзНржЯ:
```python
# test_installation.py
from playwright.sync_api import sync_playwright

def test_playwright():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()
        page.goto("https://example.com")
        print(f"Page title: {page.title()}")
        browser.close()
        print("тЬЕ Playwright successfully installed!")

if __name__ == "__main__":
    test_playwright()
```

### ЁЯЪА ржкрзНрж░ржержо Playwright ржкрзНрж░рзЛржЧрзНрж░рж╛ржо:
```python
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    # Browser launch ржХрж░рзБржи
    browser = p.chromium.launch(headless=False)  # headless=True ржХрж░рж▓рзЗ browser ржжрзЗржЦрж╛ ржпрж╛ржмрзЗ ржирж╛

    # ржирждрзБржи page рждрзИрж░рж┐ ржХрж░рзБржи
    page = browser.new_page()

    # Website ржП ржпрж╛ржи
    page.goto("https://example.com")

    # Page title print ржХрж░рзБржи
    print(f"Title: {page.title()}")

    # Browser ржмржирзНржз ржХрж░рзБржи
    browser.close()
```

---

## ЁЯФз ржбрзЗржнрзЗрж▓ржкрж╛рж░ ржЯрзБрж▓рж╕ (DevTools) ржорж╛рж╕рзНржЯрж╛рж░рж┐ {#devtools-mastery}

### ЁЯЦ▒я╕П DevTools ржЦрзЛрж▓рж╛рж░ ржЙржкрж╛ржпрж╝:
- **Windows/Linux:** `F12` ржмрж╛ `Ctrl+Shift+I`
- **Mac:** `Cmd+Option+I`
- **Right Click:** "Inspect Element"
- **Playwright ржерзЗржХрзЗ:** `args=["--auto-open-devtools-for-tabs"]`

### ЁЯУ▒ DevTools ржПрж░ ржкрзНрж░ржзрж╛ржи ржЯрзНржпрж╛ржмрж╕ржорзВрж╣:

#### 1я╕ПтГг **Elements Tab** (HTML/CSS Analysis)
```html
<!-- ржПржнрж╛ржмрзЗ HTML structure ржжрзЗржЦрждрзЗ ржкрж╛ржмрзЗржи -->
<div class="product" data-id="123">
    <h2 class="title">ржкржгрзНржпрзЗрж░ ржирж╛ржо</h2>
    <span class="price" data-currency="BDT">рзлрзжрзж ржЯрж╛ржХрж╛</span>
    <button class="add-to-cart" onclick="addToCart(123)">Add to Cart</button>
</div>
```

**ЁЯОп Advanced Element Inspection:**
- **Element select:** `Ctrl+Shift+C` (Windows) / `Cmd+Shift+C` (Mac)
- **CSS properties:** Right panel ржП "Styles", "Computed", "Layout"
- **Selector copy:** Right click тЖТ Copy тЖТ Copy selector/XPath/JS path
- **Event listeners:** "Event Listeners" tab ржП click events ржжрзЗржЦрзБржи
- **Accessibility:** "Accessibility" tab ржП screen reader info

#### 2я╕ПтГг **Console Tab** (JavaScript Testing Ground)
```javascript
// Console ржП ржПржЗ advanced ржХржорж╛ржирзНржбржЧрзБрж▓рзЛ ржЪрж╛рж▓рж┐ржпрж╝рзЗ ржжрзЗржЦрзБржи

// Basic DOM queries
document.title                           // ржкрзЗржЬрзЗрж░ ржЯрж╛ржЗржЯрзЗрж▓
document.querySelector('h1')             // ржкрзНрж░ржержо h1 element
document.querySelectorAll('.price')      // рж╕ржм price class elements
document.getElementById('main-content')   // ID ржжрж┐ржпрж╝рзЗ element

// Advanced queries
$('h1')                                  // jQuery-style selector (Chrome DevTools)
$$('.product')                           // рж╕ржм .product elements (Chrome DevTools)
$x('//div[@class="product"]')            // XPath selector

// Element properties
let element = document.querySelector('.price');
element.textContent                      // Text content
element.innerHTML                        // HTML content
element.getAttribute('data-currency')    // Attribute value
element.style.color = 'red'             // Style change

// Page manipulation
window.scrollTo(0, document.body.scrollHeight)  // Scroll to bottom
localStorage.getItem('user_data')        // Local storage data
sessionStorage.setItem('test', 'value')  // Session storage
document.cookie                          // All cookies

// Network and performance
performance.now()                        // Current timestamp
navigator.userAgent                      // Browser info
window.location.href                     // Current URL
```

#### 3я╕ПтГг **Sources Tab** (JavaScript Debugging)
- **File explorer:** рж╕ржм JavaScript, CSS files
- **Breakpoints:** Code execution ржерж╛ржорж╛ржирзЛрж░ ржЬржирзНржп
- **Watch expressions:** Variable values monitor ржХрж░рж╛
- **Call stack:** Function call hierarchy
- **Scope:** Current scope ржПрж░ variables

**ЁЯФН Debugging ржЯрж┐ржкрж╕:**
```javascript
// Console ржП debugging commands
debugger;                    // Code ржП breakpoint set ржХрж░рж╛
console.log('Debug info');   // Simple logging
console.table(data);         // Table format ржП data
console.time('operation');   // Performance timing start
console.timeEnd('operation'); // Performance timing end
```

---

## ЁЯУб ржирзЗржЯржУржпрж╝рж╛рж░рзНржХ ржЯрзНржпрж╛ржм ржоржирж┐ржЯрж░рж┐ржВ {#network-monitoring}

### ЁЯМР Network Tab ржХрзЗржи ржЕрждрзНржпржирзНржд ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг?
Network Tab рж╣рж▓рзЛ web scraping ржПрж░ **рж╕ржмржЪрзЗржпрж╝рзЗ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА ржЯрзБрж▓**ред ржПржЦрж╛ржирзЗ ржЖржкржирж┐ ржжрзЗржЦрждрзЗ ржкрж╛ржмрзЗржи:
- ЁЯФН **API Endpoints:** Hidden API calls
- ЁЯУб **AJAX Requests:** Dynamic content loading
- ЁЯНк **Cookies & Headers:** Authentication info
- ЁЯУК **Response Data:** Raw JSON/XML data
- тП▒я╕П **Timing Info:** Performance metrics

### ЁЯФН Network Tab Advanced ржмрзНржпржмрж╣рж╛рж░:

#### ржзрж╛ржк рзз: Network Tab Setup
```javascript
// DevTools ржЦрзЛрж▓рж╛рж░ ржкрж░ Console ржП ржПржЗ command ржЪрж╛рж▓рж╛ржи
// Network requests clear ржХрж░рждрзЗ
console.clear();

// ржЕржержмрж╛ Playwright ржжрж┐ржпрж╝рзЗ auto-open
```

```python
# Playwright ржП DevTools auto-open
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch(
        headless=False,
        args=["--auto-open-devtools-for-tabs"]
    )
    page = browser.new_page()
    page.goto("https://example.com")
```

#### ржзрж╛ржк рзи: Request Types ржУ Filtering
```
ЁЯОп Advanced Filter Options:
- All: рж╕ржм requests (default)
- Fetch/XHR: API calls ржПржмржВ AJAX requests
- JS: JavaScript files
- CSS: Stylesheet files
- Img: Images (PNG, JPG, SVG)
- Media: Videos, Audio files
- Font: Web fonts (WOFF, TTF)
- Doc: HTML documents
- WS: WebSocket connections
- Manifest: PWA manifest files
```

#### ржзрж╛ржк рзй: Request Analysis (ржмрж┐рж╕рзНрждрж╛рж░рж┐ржд)
ржПржХржЯрж┐ request ржП ржХрзНрж▓рж┐ржХ ржХрж░рж▓рзЗ ржПржЗ tabs ржкрж╛ржмрзЗржи:

**ЁЯУЛ Headers Tab:**
```
General:
- Request URL: https://api.example.com/products
- Request Method: GET/POST/PUT/DELETE
- Status Code: 200 OK / 404 Not Found
- Remote Address: 192.168.1.1:443

Request Headers:
- User-Agent: Browser information
- Accept: Content types accepted
- Authorization: Bearer token/API key
- Cookie: Session cookies
- Referer: Previous page URL

Response Headers:
- Content-Type: application/json
- Set-Cookie: New cookies
- Access-Control-Allow-Origin: CORS settings
- Cache-Control: Caching rules
```

**ЁЯСБя╕П Preview Tab:**
- JSON data ржПрж░ formatted view
- Images ржПрж░ preview
- HTML content ржПрж░ rendered view

**ЁЯУД Response Tab:**
- Raw response data
- JSON, XML, HTML source code
- Binary data (images, files)

**тП▒я╕П Timing Tab:**
```
Request timing breakdown:
- Queueing: Request queue time
- Stalled: Network stack delay
- DNS Lookup: Domain resolution
- Initial Connection: TCP handshake
- SSL: SSL/TLS negotiation
- Request Sent: Upload time
- Waiting (TTFB): Time to First Byte
- Content Download: Download time
```

### ЁЯОп Playwright ржжрж┐ржпрж╝рзЗ Network Monitoring:

#### Request/Response Interception:
```python
from playwright.sync_api import sync_playwright

def monitor_network():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()

        # All network requests capture ржХрж░рж╛
        requests_log = []

        def handle_request(request):
            requests_log.append({
                'url': request.url,
                'method': request.method,
                'headers': request.headers,
                'post_data': request.post_data
            })
            print(f"ЁЯУд Request: {request.method} {request.url}")

        def handle_response(response):
            print(f"ЁЯУе Response: {response.status} {response.url}")

            # JSON response parse ржХрж░рж╛
            if 'application/json' in response.headers.get('content-type', ''):
                try:
                    json_data = response.json()
                    print(f"ЁЯУК JSON Data: {json_data}")
                except:
                    pass

        # Event listeners attach ржХрж░рж╛
        page.on('request', handle_request)
        page.on('response', handle_response)

        # Website navigate ржХрж░рж╛
        page.goto("https://example.com")
        page.wait_for_load_state('networkidle')

        # Captured requests analysis
        print(f"\nЁЯУЛ Total Requests: {len(requests_log)}")
        for req in requests_log:
            if 'api' in req['url']:
                print(f"ЁЯФН API Found: {req['url']}")

        browser.close()

monitor_network()
```

### ЁЯТб Real-World Network Analysis Example:
```python
def find_hidden_apis():
    """E-commerce site ржПрж░ hidden API ржЦрзБржБржЬрзЗ ржмрзЗрж░ ржХрж░рж╛"""

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()

        api_endpoints = []

        def capture_api_calls(response):
            url = response.url

            # API patterns ржЦрзБржБржЬрзЗ ржмрзЗрж░ ржХрж░рж╛
            api_patterns = ['/api/', '/ajax/', '.json', '/graphql', '/rest/']

            if any(pattern in url for pattern in api_patterns):
                api_info = {
                    'url': url,
                    'status': response.status,
                    'method': response.request.method,
                    'headers': dict(response.headers)
                }

                # Response data capture ржХрж░рж╛
                try:
                    if response.headers.get('content-type', '').startswith('application/json'):
                        api_info['data'] = response.json()
                except:
                    pass

                api_endpoints.append(api_info)
                print(f"ЁЯОп API Found: {url}")

        page.on('response', capture_api_calls)

        # E-commerce site navigate ржХрж░рж╛
        page.goto("https://example-shop.com")

        # Product search ржХрж░рж╛ (API calls trigger ржХрж░рж╛рж░ ржЬржирзНржп)
        search_box = page.locator('input[type="search"]')
        if search_box.is_visible():
            search_box.fill('laptop')
            search_box.press('Enter')
            page.wait_for_load_state('networkidle')

        # Results analysis
        print(f"\nЁЯФН Found {len(api_endpoints)} API endpoints:")
        for api in api_endpoints:
            print(f"  ЁЯУб {api['method']} {api['url']} - Status: {api['status']}")

        browser.close()
        return api_endpoints

# Function call ржХрж░рж╛
apis = find_hidden_apis()
```

---

## ЁЯОп HTML/CSS рж╕рж┐рж▓рзЗржХрзНржЯрж░ ржПржХрзНрж╕ржкрж╛рж░рзНржЯ {#selectors-expert}

### ЁЯУЭ HTML Structure Deep Dive:
```html
<!DOCTYPE html>
<html lang="bn">
<head>
    <title>E-commerce Product Page</title>
    <meta charset="UTF-8">
</head>
<body>
    <nav class="navbar" id="main-nav">
        <ul class="nav-links">
            <li><a href="/home" class="nav-link active">Home</a></li>
            <li><a href="/products" class="nav-link">Products</a></li>
        </ul>
    </nav>

    <main class="container">
        <section class="product-section" data-category="electronics">
            <h1 id="product-title" class="title main-title">iPhone 15 Pro</h1>
            <div class="product-info">
                <span class="price" data-currency="BDT" data-amount="120000">рзз,рзирзж,рзжрзжрзж ржЯрж╛ржХрж╛</span>
                <div class="rating" data-rating="4.5">
                    <span class="stars">тШЕтШЕтШЕтШЕтШЖ</span>
                    <span class="rating-text">(4.5/5)</span>
                </div>
            </div>

            <ul class="features-list">
                <li class="feature-item" data-feature="camera">48MP Camera</li>
                <li class="feature-item" data-feature="storage">256GB Storage</li>
                <li class="feature-item" data-feature="display">6.1" Display</li>
            </ul>

            <form class="purchase-form" id="buy-form">
                <input type="number" name="quantity" value="1" min="1" class="quantity-input">
                <button type="submit" class="btn btn-primary add-to-cart" data-product-id="12345">
                    Add to Cart
                </button>
            </form>
        </section>

        <aside class="sidebar">
            <div class="related-products">
                <h3>Related Products</h3>
                <div class="product-card" data-id="67890">
                    <img src="product2.jpg" alt="iPhone 14" class="product-image">
                    <h4 class="product-name">iPhone 14</h4>
                    <span class="product-price">рзпрзл,рзжрзжрзж ржЯрж╛ржХрж╛</span>
                </div>
            </div>
        </aside>
    </main>

    <footer class="footer">
        <p>&copy; 2024 My Shop</p>
    </footer>
</body>
</html>
```

### ЁЯОп CSS Selectors - Complete Guide:

#### 1я╕ПтГг **ржмрзЗрж╕рж┐ржХ рж╕рж┐рж▓рзЗржХрзНржЯрж░:**
```css
/* Element/Tag Selector */
h1                  тЖТ рж╕ржм h1 elements
div                 тЖТ рж╕ржм div elements
span                тЖТ рж╕ржм span elements

/* Class Selector */
.container          тЖТ class="container"
.product-info       тЖТ class="product-info"
.btn.btn-primary    тЖТ class="btn btn-primary" (multiple classes)

/* ID Selector */
#product-title      тЖТ id="product-title"
#main-nav           тЖТ id="main-nav"

/* Universal Selector */
*                   тЖТ рж╕ржм elements
```

#### 2я╕ПтГг **Attribute Selectors:**
```css
/* Attribute exists */
[data-category]     тЖТ data-category attribute ржЖржЫрзЗ
[href]              тЖТ href attribute ржЖржЫрзЗ

/* Attribute equals */
[data-currency="BDT"]     тЖТ data-currency="BDT"
[type="submit"]           тЖТ type="submit"

/* Attribute contains */
[class*="btn"]            тЖТ class ржП "btn" ржЖржЫрзЗ
[data-feature*="camera"]  тЖТ data-feature ржП "camera" ржЖржЫрзЗ

/* Attribute starts with */
[class^="product"]        тЖТ class "product" ржжрж┐ржпрж╝рзЗ рж╢рзБрж░рзБ
[href^="https"]           тЖТ href "https" ржжрж┐ржпрж╝рзЗ рж╢рзБрж░рзБ

/* Attribute ends with */
[class$="primary"]        тЖТ class "primary" ржжрж┐ржпрж╝рзЗ рж╢рзЗрж╖
[src$=".jpg"]             тЖТ src ".jpg" ржжрж┐ржпрж╝рзЗ рж╢рзЗрж╖
```

#### 3я╕ПтГг **Combinators (рж╕ржорзНржкрж░рзНржХ ржирж┐рж░рзНржжрзЗрж╢ржХ):**
```css
/* Descendant (ржмржВрж╢ржзрж░) */
.container h1             тЖТ container ржПрж░ ржнрж┐рждрж░рзЗрж░ ржпрзЗржХрзЛржирзЛ h1
.product-section span     тЖТ product-section ржПрж░ ржнрж┐рждрж░рзЗрж░ ржпрзЗржХрзЛржирзЛ span

/* Child (рж╕ржирзНрждрж╛ржи) */
.container > h1           тЖТ container ржПрж░ direct child h1
.nav-links > li           тЖТ nav-links ржПрж░ direct child li

/* Adjacent Sibling (ржкрж╛рж╢рзЗрж░ ржнрж╛ржЗ) */
h1 + div                  тЖТ h1 ржПрж░ ржарж┐ржХ ржкрж░рзЗрж░ div
.price + .rating          тЖТ price ржПрж░ ржарж┐ржХ ржкрж░рзЗрж░ rating

/* General Sibling (рж╕ржм ржнрж╛ржЗ) */
h1 ~ div                  тЖТ h1 ржПрж░ ржкрж░рзЗрж░ рж╕ржм div
.price ~ span             тЖТ price ржПрж░ ржкрж░рзЗрж░ рж╕ржм span
```

#### 4я╕ПтГг **Pseudo Selectors:**
```css
/* Structural Pseudo-classes */
li:first-child            тЖТ ржкрзНрж░ржержо li
li:last-child             тЖТ рж╢рзЗрж╖ li
li:nth-child(2)           тЖТ рзиржпрж╝ li
li:nth-child(odd)         тЖТ ржмрж┐ржЬрзЛржбрж╝ position ржПрж░ li
li:nth-child(even)        тЖТ ржЬрзЛржбрж╝ position ржПрж░ li
li:nth-child(3n+1)        тЖТ рзйn+рзз position ржПрж░ li

/* State Pseudo-classes */
a:hover                   тЖТ mouse hover ржХрж░рж▓рзЗ
input:focus               тЖТ input field focus ржХрж░рж▓рзЗ
button:disabled           тЖТ disabled button
input:checked             тЖТ checked checkbox/radio

/* Content Pseudo-classes */
p:empty                   тЖТ ржЦрж╛рж▓рж┐ p elements
div:not(.sidebar)         тЖТ sidebar class ржирзЗржЗ ржПржоржи div
```

#### 5я╕ПтГг **Advanced Selectors:**
```css
/* Multiple Selectors */
h1, h2, h3                тЖТ рж╕ржм h1, h2, h3
.price, .rating           тЖТ price ржЕржержмрж╛ rating class

/* Complex Combinations */
.product-section .price:not(.discount)    тЖТ product-section ржПрж░ ржнрж┐рждрж░рзЗрж░ price class ржпрж╛рж░ discount class ржирзЗржЗ
.container > .product-info span[data-currency] тЖТ container ржПрж░ direct child product-info ржПрж░ span ржпрж╛рж░ data-currency ржЖржЫрзЗ

/* Case-insensitive Attribute */
[data-feature="CAMERA" i] тЖТ case-insensitive matching
```

### ЁЯОн Playwright ржП Selector ржмрзНржпржмрж╣рж╛рж░:

#### Text-based Selectors (Playwright Special):
```python
# Text content ржжрж┐ржпрж╝рзЗ select ржХрж░рж╛
page.locator('text="Add to Cart"')           # Exact text match
page.locator('text=/Add.*Cart/i')            # Regex match (case-insensitive)
page.locator('"Add to Cart"')                # Shorthand for exact text

# Partial text match
page.locator('text="Add"')                   # "Add" text ржЖржЫрзЗ ржПржоржи element

# Role-based selectors (Accessibility)
page.locator('role=button[name="Add to Cart"]')  # Button role with name
page.locator('role=textbox[name="Search"]')      # Textbox role
page.locator('role=link[name="Home"]')           # Link role
```

#### XPath Selectors:
```python
# XPath ржжрж┐ржпрж╝рзЗ complex selection
page.locator('xpath=//div[@class="product-info"]//span[@data-currency="BDT"]')
page.locator('xpath=//button[contains(text(), "Add")]')
page.locator('xpath=//li[position()=2]')     # рзиржпрж╝ li element
page.locator('xpath=//div[following-sibling::span[@class="price"]]')  # price class ржПрж░ ржЖржЧрзЗрж░ div
```

### ЁЯТб Selector Testing ржУ Debugging:

#### DevTools Console ржП Test ржХрж░рж╛:
```javascript
// CSS Selector test
document.querySelector('.product-info .price')
document.querySelectorAll('.feature-item')

// XPath test
$x('//div[@class="product-info"]//span[@data-currency="BDT"]')

// Playwright-style text selector simulation
document.evaluate('//button[contains(text(), "Add to Cart")]', document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue
```

#### Playwright ржП Selector Debugging:
```python
# Selector highlight ржХрж░рж╛
page.locator('.price').highlight()

# Element count check ржХрж░рж╛
count = page.locator('.feature-item').count()
print(f"Found {count} feature items")

# Wait for selector
page.wait_for_selector('.product-info', timeout=5000)

# Check if selector exists
if page.locator('.add-to-cart').is_visible():
    print("Add to cart button is visible")
```

---

## ЁЯОн Playwright ржмрзЗрж╕рж┐ржХ - Browser ржУ Page {#playwright-basics}

### ЁЯЪА Browser Launch Options:

#### Basic Browser Launch:
```python
from playwright.sync_api import sync_playwright

# рж╕ржмржЪрзЗржпрж╝рзЗ рж╕рж┐ржорзНржкрж▓ way
with sync_playwright() as p:
    browser = p.chromium.launch()  # Headless mode (default)
    page = browser.new_page()
    page.goto("https://example.com")
    print(page.title())
    browser.close()
```

#### Advanced Browser Configuration:
```python
def advanced_browser_setup():
    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=False,                    # Browser ржжрзЗржЦрж╛ ржпрж╛ржмрзЗ
            slow_mo=1000,                      # ржкрзНрж░рждрж┐ржЯрж┐ action ржП 1 рж╕рзЗржХрзЗржирзНржб delay
            devtools=True,                     # DevTools auto open
            args=[
                "--start-maximized",           # Full screen
                "--disable-blink-features=AutomationControlled",  # Bot detection ржПржбрж╝рж╛ржирзЛ
                "--disable-web-security",      # CORS disable
                "--disable-features=VizDisplayCompositor",
                "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            ]
        )

        # Browser context with custom settings
        context = browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            locale='bn-BD',                    # Bengali locale
            timezone_id='Asia/Dhaka',          # Dhaka timezone
            permissions=['geolocation'],       # Location permission
            geolocation={'latitude': 23.8103, 'longitude': 90.4125}  # Dhaka coordinates
        )

        page = context.new_page()
        return browser, context, page

browser, context, page = advanced_browser_setup()
```

### ЁЯМР Page Navigation ржУ Loading:

#### Basic Navigation:
```python
# URL ржП ржпрж╛ржУржпрж╝рж╛
page.goto("https://example.com")

# Wait strategies
page.wait_for_load_state("load")          # HTML load complete
page.wait_for_load_state("domcontentloaded")  # DOM ready
page.wait_for_load_state("networkidle")   # Network idle (recommended)

# Page info
print(f"Title: {page.title()}")
print(f"URL: {page.url}")
print(f"Content: {page.content()[:100]}...")  # First 100 chars
```

#### Advanced Navigation:
```python
def smart_navigation(page, url, max_retries=3):
    """Smart navigation with retry logic"""

    for attempt in range(max_retries):
        try:
            # Navigate with timeout
            response = page.goto(url, timeout=30000, wait_until="networkidle")

            # Check response status
            if response.status >= 400:
                print(f"тЭМ HTTP Error: {response.status}")
                continue

            # Wait for critical elements
            page.wait_for_selector('body', timeout=10000)

            # Check if page loaded properly
            if page.locator('body').is_visible():
                print(f"тЬЕ Successfully loaded: {url}")
                return True

        except Exception as e:
            print(f"тЪая╕П Attempt {attempt + 1} failed: {e}")
            if attempt < max_retries - 1:
                page.wait_for_timeout(2000)  # Wait before retry
                continue

    print(f"тЭМ Failed to load {url} after {max_retries} attempts")
    return False

# Usage
success = smart_navigation(page, "https://example.com")
```

### ЁЯФН Element Location ржУ Selection:

#### Modern Locator API:
```python
# Modern way (recommended)
title = page.locator("h1")                    # CSS selector
price = page.locator(".price")                # Class selector
button = page.locator("#submit-btn")          # ID selector

# Text-based locators
add_button = page.locator('text="Add to Cart"')
link = page.locator('text=/Home|рж╣рзЛржо/i')       # Regex with Bengali

# Role-based locators (Accessibility)
search_box = page.locator('role=textbox[name="Search"]')
nav_links = page.locator('role=link')

# Attribute-based locators
product = page.locator('[data-product-id="123"]')
images = page.locator('img[alt*="product"]')
```

#### Legacy Selectors (still useful):
```python
# Old way (still works)
title_element = page.query_selector("h1")
all_links = page.query_selector_all("a")

# XPath
xpath_element = page.locator('xpath=//div[@class="product"]//span[@class="price"]')
```

### ЁЯУК Element Information Extraction:

#### Text Content:
```python
# Text content ржирзЗржУржпрж╝рж╛
title_text = page.locator("h1").text_content()
price_text = page.locator(".price").text_content()

# Inner text (visible text only)
visible_text = page.locator(".description").inner_text()

# Inner HTML
html_content = page.locator(".product-info").inner_html()

# All text contents (multiple elements)
all_prices = page.locator(".price").all_text_contents()
print(f"All prices: {all_prices}")
```

#### Attributes:
```python
# Single attribute
link_href = page.locator("a").get_attribute("href")
image_src = page.locator("img").get_attribute("src")
data_id = page.locator(".product").get_attribute("data-id")

# Multiple attributes
element = page.locator(".product-card")
attributes = {
    'id': element.get_attribute('id'),
    'class': element.get_attribute('class'),
    'data-price': element.get_attribute('data-price')
}
print(attributes)
```

#### Element Properties:
```python
# Element visibility
is_visible = page.locator(".popup").is_visible()
is_hidden = page.locator(".loading").is_hidden()

# Element state
is_enabled = page.locator("button").is_enabled()
is_disabled = page.locator("input").is_disabled()
is_checked = page.locator("checkbox").is_checked()

# Element count
product_count = page.locator(".product-card").count()
print(f"Found {product_count} products")
```

### ЁЯОп Multiple Elements Handling:

#### Working with Lists:
```python
# Get all matching elements
products = page.locator(".product-card").all()

# Loop through elements
for i, product in enumerate(products):
    name = product.locator(".product-name").text_content()
    price = product.locator(".product-price").text_content()
    print(f"{i+1}. {name} - {price}")

# Nth element
first_product = page.locator(".product-card").nth(0)  # First
last_product = page.locator(".product-card").nth(-1)  # Last
second_product = page.locator(".product-card").nth(1) # Second

# Filter elements
expensive_products = page.locator(".product-card").filter(has_text="рзлрзж,рзжрзжрзж")
available_products = page.locator(".product-card").filter(has=page.locator(".in-stock"))
```

#### Advanced Element Queries:
```python
def extract_product_data(page):
    """Extract all product data from a page"""

    products = []
    product_cards = page.locator(".product-card").all()

    for card in product_cards:
        try:
            product_data = {
                'name': card.locator(".product-name").text_content().strip(),
                'price': card.locator(".product-price").text_content().strip(),
                'rating': card.locator(".rating").text_content().strip() if card.locator(".rating").is_visible() else "No rating",
                'image_url': card.locator("img").get_attribute("src"),
                'product_url': card.locator("a").get_attribute("href"),
                'availability': "In Stock" if card.locator(".in-stock").is_visible() else "Out of Stock"
            }
            products.append(product_data)

        except Exception as e:
            print(f"Error extracting product data: {e}")
            continue

    return products

# Usage
products = extract_product_data(page)
print(f"Extracted {len(products)} products")
```

---

## ЁЯЦ▒я╕П Element Interaction - Click, Type, Fill {#element-interaction}

### ЁЯОп Click Operations:

#### Basic Clicking:
```python
# Simple click
page.locator("button").click()
page.locator("#submit-btn").click()
page.locator('text="Add to Cart"').click()

# Click with options
page.locator("button").click(
    button="left",           # left, right, middle
    click_count=1,           # Single click (default)
    delay=100,               # Delay between mousedown and mouseup
    force=True,              # Force click even if element is not visible
    no_wait_after=False,     # Wait for navigation after click
    timeout=30000            # Timeout in milliseconds
)

# Double click
page.locator(".file-item").dblclick()

# Right click (context menu)
page.locator(".item").click(button="right")
```

#### Advanced Click Scenarios:
```python
def smart_click(page, selector, max_attempts=3):
    """Smart clicking with retry logic"""

    for attempt in range(max_attempts):
        try:
            element = page.locator(selector)

            # Wait for element to be visible and enabled
            element.wait_for(state="visible", timeout=10000)

            # Scroll into view if needed
            element.scroll_into_view_if_needed()

            # Check if clickable
            if element.is_enabled():
                element.click()
                print(f"тЬЕ Successfully clicked: {selector}")
                return True
            else:
                print(f"тЪая╕П Element not enabled: {selector}")

        except Exception as e:
            print(f"тЭМ Click attempt {attempt + 1} failed: {e}")
            if attempt < max_attempts - 1:
                page.wait_for_timeout(1000)
                continue

    return False

# Usage
success = smart_click(page, ".add-to-cart-btn")
```

### тМия╕П Text Input Operations:

#### Basic Text Input:
```python
# Fill input field (clears first, then types)
page.locator('input[name="username"]').fill("my_username")
page.locator('#password').fill("my_password")
page.locator('textarea').fill("This is a long text message")

# Type text (simulates human typing)
page.locator('input[type="search"]').type("laptop computer", delay=100)

# Clear input
page.locator('input').clear()

# Press specific keys
page.locator('input').press("Enter")
page.locator('input').press("Tab")
page.locator('input').press("Escape")
page.locator('input').press("Control+A")  # Select all
```

#### Advanced Text Input:
```python
def fill_form_data(page, form_data):
    """Fill multiple form fields"""

    for field_name, value in form_data.items():
        try:
            # Try different selector patterns
            selectors = [
                f'input[name="{field_name}"]',
                f'#{field_name}',
                f'.{field_name}',
                f'input[placeholder*="{field_name}"]'
            ]

            element_found = False
            for selector in selectors:
                element = page.locator(selector)
                if element.is_visible():
                    element.fill(str(value))
                    print(f"тЬЕ Filled {field_name}: {value}")
                    element_found = True
                    break

            if not element_found:
                print(f"тЭМ Field not found: {field_name}")

        except Exception as e:
            print(f"тЭМ Error filling {field_name}: {e}")

# Usage
form_data = {
    "username": "john_doe",
    "email": "john@example.com",
    "phone": "01712345678",
    "address": "Dhaka, Bangladesh"
}

fill_form_data(page, form_data)
```

### ЁЯУЛ Form Interactions:

#### Dropdown/Select Operations:
```python
# Select by value
page.locator('select[name="country"]').select_option("BD")

# Select by label
page.locator('select').select_option(label="Bangladesh")

# Select by index
page.locator('select').select_option(index=2)

# Multiple selections
page.locator('select[multiple]').select_option(["option1", "option2"])

# Get selected value
selected = page.locator('select').input_value()
print(f"Selected: {selected}")
```

#### Checkbox and Radio Operations:
```python
# Check checkbox
page.locator('input[type="checkbox"]').check()

# Uncheck checkbox
page.locator('input[type="checkbox"]').uncheck()

# Set checkbox state
page.locator('input[type="checkbox"]').set_checked(True)

# Radio button selection
page.locator('input[value="male"]').check()

# Check if checked
is_checked = page.locator('input[type="checkbox"]').is_checked()
print(f"Checkbox is checked: {is_checked}")
```

#### File Upload:
```python
# Single file upload
page.locator('input[type="file"]').set_input_files("path/to/file.pdf")

# Multiple files upload
page.locator('input[type="file"]').set_input_files([
    "file1.jpg",
    "file2.png",
    "document.pdf"
])

# Remove files
page.locator('input[type="file"]').set_input_files([])
```

### ЁЯЦ▒я╕П Mouse Operations:

#### Hover and Mouse Events:
```python
# Hover over element
page.locator(".menu-item").hover()

# Hover with custom position
page.locator(".element").hover(position={"x": 50, "y": 50})

# Mouse down and up
page.locator(".draggable").mouse_down()
page.locator(".drop-zone").mouse_up()

# Drag and drop
page.locator(".source").drag_to(page.locator(".target"))
```

### тП▒я╕П Wait Strategies:

#### Element-based Waits:
```python
# Wait for element to be visible
page.locator(".loading").wait_for(state="visible")

# Wait for element to be hidden
page.locator(".popup").wait_for(state="hidden")

# Wait for element to be attached to DOM
page.locator(".dynamic-content").wait_for(state="attached")

# Wait for element to be detached from DOM
page.locator(".temporary-element").wait_for(state="detached")

# Wait with timeout
try:
    page.locator(".slow-element").wait_for(state="visible", timeout=5000)
except TimeoutError:
    print("Element did not appear within 5 seconds")
```

#### Custom Wait Conditions:
```python
def wait_for_element_count(page, selector, expected_count, timeout=10000):
    """Wait until element count matches expected"""

    def check_count():
        actual_count = page.locator(selector).count()
        return actual_count == expected_count

    page.wait_for_function(
        f"() => document.querySelectorAll('{selector}').length === {expected_count}",
        timeout=timeout
    )

# Wait for specific text to appear
def wait_for_text(page, selector, expected_text, timeout=10000):
    """Wait for element to contain specific text"""

    page.wait_for_function(
        f"""() => {{
            const element = document.querySelector('{selector}');
            return element && element.textContent.includes('{expected_text}');
        }}""",
        timeout=timeout
    )

# Usage
wait_for_element_count(page, ".product-card", 20)
wait_for_text(page, ".status", "Loading complete")
```

### ЁЯОп Complex Interaction Scenarios:

#### Login Automation:
```python
def automated_login(page, username, password):
    """Automated login with error handling"""

    try:
        # Navigate to login page
        page.goto("https://example.com/login")

        # Wait for login form
        page.wait_for_selector('form', timeout=10000)

        # Fill credentials
        page.locator('input[name="username"]').fill(username)
        page.locator('input[name="password"]').fill(password)

        # Submit form
        page.locator('button[type="submit"]').click()

        # Wait for navigation or error message
        try:
            # Success: wait for dashboard
            page.wait_for_url("**/dashboard", timeout=5000)
            print("тЬЕ Login successful!")
            return True

        except:
            # Check for error message
            error_element = page.locator('.error-message')
            if error_element.is_visible():
                error_text = error_element.text_content()
                print(f"тЭМ Login failed: {error_text}")
            else:
                print("тЭМ Login failed: Unknown error")
            return False

    except Exception as e:
        print(f"тЭМ Login error: {e}")
        return False

# Usage
success = automated_login(page, "my_username", "my_password")
```

#### Search and Filter:
```python
def search_and_filter(page, search_term, filters=None):
    """Search with filters and return results"""

    # Perform search
    search_box = page.locator('input[type="search"]')
    search_box.fill(search_term)
    search_box.press("Enter")

    # Wait for results
    page.wait_for_selector('.search-results', timeout=10000)

    # Apply filters if provided
    if filters:
        for filter_type, filter_value in filters.items():
            filter_selector = f'select[name="{filter_type}"]'
            if page.locator(filter_selector).is_visible():
                page.locator(filter_selector).select_option(filter_value)
                page.wait_for_load_state('networkidle')

    # Extract results
    results = []
    result_elements = page.locator('.search-result').all()

    for element in result_elements:
        result_data = {
            'title': element.locator('.title').text_content(),
            'description': element.locator('.description').text_content(),
            'url': element.locator('a').get_attribute('href')
        }
        results.append(result_data)

    return results

# Usage
filters = {"category": "electronics", "price_range": "10000-50000"}
results = search_and_filter(page, "smartphone", filters)
print(f"Found {len(results)} results")
```
```

### ЁЯУЪ Requests + BeautifulSoup (ржмрзЗрж╕рж┐ржХ):

#### рж╕рж┐ржорзНржкрж▓ рж╕рзНржХрзНрж░рзНржпрж╛ржкрж┐ржВ:
```python
import requests
from bs4 import BeautifulSoup
import time

def scrape_basic_site():
    url = "https://quotes.toscrape.com/"
    
    # Headers ржпрзЛржЧ ржХрж░рж╛ (bot detection ржПржбрж╝рж╛ржирзЛрж░ ржЬржирзНржп)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    response = requests.get(url, headers=headers)
    
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # рж╕ржм quotes рж╕ржВржЧрзНрж░рж╣
        quotes = soup.find_all('div', class_='quote')
        
        for quote in quotes:
            text = quote.find('span', class_='text').text
            author = quote.find('small', class_='author').text
            tags = [tag.text for tag in quote.find_all('a', class_='tag')]
            
            print(f"Quote: {text}")
            print(f"Author: {author}")
            print(f"Tags: {', '.join(tags)}")
            print("-" * 50)
    
    else:
        print(f"Error: {response.status_code}")

# ржлрж╛ржВрж╢ржи ржЪрж╛рж▓рж╛ржирзЛ
scrape_basic_site()
```

#### Multiple Pages рж╕рзНржХрзНрж░рзНржпрж╛ржкрж┐ржВ:
```python
def scrape_multiple_pages():
    base_url = "https://quotes.toscrape.com/page/{}"
    all_quotes = []
    
    for page in range(1, 6):  # рзз-рзл ржкрзЗржЬ
        url = base_url.format(page)
        print(f"Scraping page {page}...")
        
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        quotes = soup.find_all('div', class_='quote')
        
        if not quotes:  # ржЖрж░ ржХрзЛржи quote ржирзЗржЗ
            break
            
        for quote in quotes:
            quote_data = {
                'text': quote.find('span', class_='text').text,
                'author': quote.find('small', class_='author').text,
                'tags': [tag.text for tag in quote.find_all('a', class_='tag')]
            }
            all_quotes.append(quote_data)
        
        # рж╕рж╛ржЗржЯрзЗрж░ ржЙржкрж░ ржЪрж╛ржк ржХржорж╛ржирзЛрж░ ржЬржирзНржп delay
        time.sleep(1)
    
    return all_quotes

quotes = scrape_multiple_pages()
print(f"Total quotes collected: {len(quotes)}")
```

### ЁЯФД Session ржмрзНржпржмрж╣рж╛рж░ (Login рж╕рж╣):
```python
import requests
from bs4 import BeautifulSoup

def login_and_scrape():
    session = requests.Session()

    # Login page ржерзЗржХрзЗ CSRF token ржирзЗржУржпрж╝рж╛
    login_url = "https://example.com/login"
    login_page = session.get(login_url)
    soup = BeautifulSoup(login_page.content, 'html.parser')
    csrf_token = soup.find('input', {'name': 'csrf_token'})['value']

    # Login data
    login_data = {
        'username': 'your_username',
        'password': 'your_password',
        'csrf_token': csrf_token
    }

    # Login ржХрж░рж╛
    session.post(login_url, data=login_data)

    # Protected page access
    protected_url = "https://example.com/protected"
    response = session.get(protected_url)

    if "dashboard" in response.text.lower():
        print("Successfully logged in!")
        # ржПржЦржи scraping ржХрж░рзБржи
    else:
        print("Login failed!")

login_and_scrape()
```

### ЁЯУК Error Handling ржУ Retry Logic:
```python
import requests
from bs4 import BeautifulSoup
import time
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

def create_session_with_retries():
    session = requests.Session()

    # Retry strategy
    retry_strategy = Retry(
        total=3,                    # ржорзЛржЯ рзй ржмрж╛рж░ ржЪрзЗрж╖рзНржЯрж╛
        backoff_factor=1,           # ржкрзНрж░рждрж┐ржмрж╛рж░ рзз рж╕рзЗржХрзЗржирзНржб ржмрзЗрж╢рж┐ ржЕржкрзЗржХрзНрж╖рж╛
        status_forcelist=[429, 500, 502, 503, 504],  # ржПржЗ status codes ржП retry
    )

    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)

    return session

def robust_scraping():
    session = create_session_with_retries()

    urls = [
        "https://example1.com",
        "https://example2.com",
        "https://example3.com"
    ]

    for url in urls:
        try:
            response = session.get(url, timeout=10)
            response.raise_for_status()  # HTTP error ржерж╛ржХрж▓рзЗ exception raise ржХрж░ржмрзЗ

            soup = BeautifulSoup(response.content, 'html.parser')
            # Scraping logic here

        except requests.exceptions.RequestException as e:
            print(f"Error scraping {url}: {e}")
            continue

        time.sleep(2)  # Rate limiting

robust_scraping()
```

---

## ЁЯОн Playwright ржжрж┐ржпрж╝рзЗ ржЕрзНржпрж╛ржбржнрж╛ржирзНрж╕ рж╕рзНржХрзНрж░рзНржпрж╛ржкрж┐ржВ {#playwright-advanced}

### ЁЯЪА Playwright ржХрзЗржи ржмрзНржпржмрж╣рж╛рж░ ржХрж░ржмрзЗржи?
- **JavaScript рж░рзЗржирзНржбрж╛рж░рж┐ржВ:** Dynamic content load рж╣ржпрж╝
- **User Interaction:** Click, scroll, form fill ржХрж░рждрзЗ ржкрж╛рж░рзЗржи
- **Multiple Browsers:** Chrome, Firefox, Safari support
- **Mobile Emulation:** Mobile device simulate ржХрж░рждрзЗ ржкрж╛рж░рзЗржи

### ЁЯОп ржмрзЗрж╕рж┐ржХ Playwright Setup:
```python
from playwright.sync_api import sync_playwright
import time

def basic_playwright_scraping():
    with sync_playwright() as p:
        # Browser launch (headless=False ржорж╛ржирзЗ browser ржжрзЗржЦрж╛ ржпрж╛ржмрзЗ)
        browser = p.chromium.launch(
            headless=False,
            args=["--auto-open-devtools-for-tabs"]
        )

        # New page create
        page = browser.new_page()

        # URL ржП ржпрж╛ржУржпрж╝рж╛
        page.goto("https://example.com")

        # Page load рж╣ржУржпрж╝рж╛рж░ ржЬржирзНржп ржЕржкрзЗржХрзНрж╖рж╛
        page.wait_for_load_state("networkidle")

        # Title print
        print(f"Page title: {page.title()}")

        # Element ржерзЗржХрзЗ text ржирзЗржУржпрж╝рж╛
        heading = page.locator("h1").text_content()
        print(f"Main heading: {heading}")

        # Screenshot ржирзЗржУржпрж╝рж╛
        page.screenshot(path="screenshot.png")

        browser.close()

basic_playwright_scraping()
```

### ЁЯЦ▒я╕П User Interactions:
```python
def interactive_scraping():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()

        page.goto("https://example.com/search")

        # Search box ржП text type ржХрж░рж╛
        page.fill('input[name="search"]', 'Python programming')

        # Search button ржП click
        page.click('button[type="submit"]')

        # Results load рж╣ржУржпрж╝рж╛рж░ ржЬржирзНржп ржЕржкрзЗржХрзНрж╖рж╛
        page.wait_for_selector('.search-results')

        # Results scrape ржХрж░рж╛
        results = page.locator('.search-result').all()

        for i, result in enumerate(results):
            title = result.locator('.title').text_content()
            link = result.locator('a').get_attribute('href')
            print(f"{i+1}. {title} - {link}")

        browser.close()

interactive_scraping()
```

### ЁЯУ▒ Mobile Device Emulation:
```python
def mobile_scraping():
    with sync_playwright() as p:
        # Mobile device emulate ржХрж░рж╛
        iphone = p.devices['iPhone 12']
        browser = p.chromium.launch()
        context = browser.new_context(**iphone)
        page = context.new_page()

        page.goto("https://example.com")

        # Mobile specific elements scrape ржХрж░рж╛
        mobile_menu = page.locator('.mobile-menu')
        if mobile_menu.is_visible():
            mobile_menu.click()

        browser.close()

mobile_scraping()
```

### ЁЯФД Infinite Scroll Handling:
```python
def scrape_infinite_scroll():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()

        page.goto("https://example.com/infinite-scroll")

        # Initial items count
        previous_count = 0

        while True:
            # Current items count
            current_count = page.locator('.item').count()

            if current_count == previous_count:
                print("No more items to load")
                break

            print(f"Loaded {current_count} items")
            previous_count = current_count

            # Scroll to bottom
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")

            # Wait for new content
            page.wait_for_timeout(2000)

        # Scrape all items
        items = page.locator('.item').all()
        for item in items:
            title = item.locator('.title').text_content()
            print(title)

        browser.close()

scrape_infinite_scroll()
```

### ЁЯНк Cookies ржУ Local Storage:
```python
def handle_cookies_storage():
    with sync_playwright() as p:
        browser = p.chromium.launch()
        context = browser.new_context()
        page = context.new_page()

        # Cookie set ржХрж░рж╛
        context.add_cookies([{
            'name': 'session_id',
            'value': 'abc123',
            'domain': 'example.com',
            'path': '/'
        }])

        page.goto("https://example.com")

        # Local storage ржП data set ржХрж░рж╛
        page.evaluate("""
            localStorage.setItem('user_preference', 'dark_mode');
        """)

        # Cookie read ржХрж░рж╛
        cookies = context.cookies()
        for cookie in cookies:
            print(f"{cookie['name']}: {cookie['value']}")

        # Local storage read ржХрж░рж╛
        user_pref = page.evaluate("localStorage.getItem('user_preference')")
        print(f"User preference: {user_pref}")

        browser.close()

handle_cookies_storage()
```

### ЁЯЪл Request Blocking (Ad Block):
```python
def block_ads_and_trackers():
    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()

        # Block ржХрж░рж╛рж░ domain list
        blocked_domains = [
            "doubleclick.net",
            "googlesyndication.com",
            "googletagmanager.com",
            "facebook.com/tr",
            "google-analytics.com"
        ]

        def handle_route(route, request):
            # Check if request URL contains blocked domains
            if any(domain in request.url for domain in blocked_domains):
                print(f"Blocked: {request.url}")
                route.abort()
            else:
                route.continue_()

        # All requests intercept ржХрж░рж╛
        page.route("**/*", handle_route)

        page.goto("https://example.com")

        # Page content scrape ржХрж░рж╛ (ads ржЫрж╛ржбрж╝рж╛)
        content = page.locator('.main-content').text_content()
        print(content)

        browser.close()

block_ads_and_trackers()
```

---

## ЁЯТ╛ ржбрзЗржЯрж╛ рж╕ржВрж░ржХрзНрж╖ржг ржУ ржкрзНрж░рж╕рзЗрж╕рж┐ржВ {#data-processing}

### ЁЯУК CSV ржлрж╛ржЗрж▓рзЗ рж╕рзЗржн ржХрж░рж╛:
```python
import csv
import pandas as pd

def save_to_csv(data, filename):
    """
    data: list of dictionaries
    filename: CSV file name
    """
    if not data:
        print("No data to save!")
        return

    # CSV writer ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ
    with open(filename, 'w', newline='', encoding='utf-8') as file:
        fieldnames = data[0].keys()
        writer = csv.DictWriter(file, fieldnames=fieldnames)

        writer.writeheader()
        writer.writerows(data)

    print(f"Data saved to {filename}")

# Example usage
scraped_data = [
    {'name': 'Product 1', 'price': 'рзлрзжрзж ржЯрж╛ржХрж╛', 'rating': '4.5'},
    {'name': 'Product 2', 'price': 'рзнрзжрзж ржЯрж╛ржХрж╛', 'rating': '4.2'},
]

save_to_csv(scraped_data, 'products.csv')
```

### ЁЯР╝ Pandas ржжрж┐ржпрж╝рзЗ ржбрзЗржЯрж╛ ржкрзНрж░рж╕рзЗрж╕рж┐ржВ:
```python
import pandas as pd
import numpy as np

def process_scraped_data():
    # CSV ржерзЗржХрзЗ data load ржХрж░рж╛
    df = pd.read_csv('products.csv')

    # Data cleaning
    df['price_numeric'] = df['price'].str.extract('(\d+)').astype(int)
    df['rating_numeric'] = df['rating'].astype(float)

    # Data analysis
    print("=== Data Summary ===")
    print(f"Total products: {len(df)}")
    print(f"Average price: {df['price_numeric'].mean():.2f} ржЯрж╛ржХрж╛")
    print(f"Average rating: {df['rating_numeric'].mean():.2f}")

    # Filter high-rated products
    high_rated = df[df['rating_numeric'] >= 4.0]
    print(f"High-rated products: {len(high_rated)}")

    # Save processed data
    df.to_csv('processed_products.csv', index=False)

    return df

processed_df = process_scraped_data()
```

### ЁЯЧДя╕П Database ржП рж╕рзЗржн ржХрж░рж╛ (SQLite):
```python
import sqlite3
import pandas as pd

def save_to_database(data, db_name='scraped_data.db'):
    # Database connection
    conn = sqlite3.connect(db_name)
    cursor = conn.cursor()

    # Table create ржХрж░рж╛
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS products (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT NOT NULL,
            price TEXT,
            rating REAL,
            scraped_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')

    # Data insert ржХрж░рж╛
    for item in data:
        cursor.execute('''
            INSERT INTO products (name, price, rating)
            VALUES (?, ?, ?)
        ''', (item['name'], item['price'], float(item['rating'])))

    conn.commit()
    conn.close()
    print(f"Data saved to database: {db_name}")

# Database ржерзЗржХрзЗ data read ржХрж░рж╛
def read_from_database(db_name='scraped_data.db'):
    conn = sqlite3.connect(db_name)
    df = pd.read_sql_query("SELECT * FROM products", conn)
    conn.close()
    return df

# Usage
scraped_data = [
    {'name': 'Product 1', 'price': 'рзлрзжрзж ржЯрж╛ржХрж╛', 'rating': '4.5'},
    {'name': 'Product 2', 'price': 'рзнрзжрзж ржЯрж╛ржХрж╛', 'rating': '4.2'},
]

save_to_database(scraped_data)
db_data = read_from_database()
print(db_data)
```

### ЁЯУИ ржбрзЗржЯрж╛ ржнрж┐ржЬрзБржпрж╝рж╛рж▓рж╛ржЗржЬрзЗрж╢ржи:
```python
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_data(df):
    # Set Bengali font (if available)
    plt.rcParams['font.family'] = ['DejaVu Sans']

    # Price distribution
    plt.figure(figsize=(12, 8))

    plt.subplot(2, 2, 1)
    plt.hist(df['price_numeric'], bins=20, alpha=0.7)
    plt.title('Price Distribution')
    plt.xlabel('Price (Taka)')
    plt.ylabel('Frequency')

    # Rating distribution
    plt.subplot(2, 2, 2)
    plt.hist(df['rating_numeric'], bins=10, alpha=0.7, color='orange')
    plt.title('Rating Distribution')
    plt.xlabel('Rating')
    plt.ylabel('Frequency')

    # Price vs Rating scatter plot
    plt.subplot(2, 2, 3)
    plt.scatter(df['price_numeric'], df['rating_numeric'], alpha=0.6)
    plt.title('Price vs Rating')
    plt.xlabel('Price (Taka)')
    plt.ylabel('Rating')

    # Top 10 products by rating
    plt.subplot(2, 2, 4)
    top_products = df.nlargest(10, 'rating_numeric')
    plt.barh(range(len(top_products)), top_products['rating_numeric'])
    plt.title('Top 10 Products by Rating')
    plt.xlabel('Rating')

    plt.tight_layout()
    plt.savefig('data_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()

# Usage
df = pd.read_csv('processed_products.csv')
visualize_data(df)
```

---

## ЁЯФз рж╕ржорж╕рзНржпрж╛ рж╕ржорж╛ржзрж╛ржи ржУ ржЯрж┐ржкрж╕ {#troubleshooting}

### ЁЯЪл рж╕рж╛ржзрж╛рж░ржг рж╕ржорж╕рзНржпрж╛ ржУ рж╕ржорж╛ржзрж╛ржи:

#### рзз. **Bot Detection ржПржбрж╝рж╛ржирзЛ:**
```python
import random
import time
from fake_useragent import UserAgent

def avoid_bot_detection():
    ua = UserAgent()

    headers = {
        'User-Agent': ua.random,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }

    # Random delay between requests
    delay = random.uniform(1, 3)
    time.sleep(delay)

    return headers

# Playwright ржП User-Agent change ржХрж░рж╛
def playwright_avoid_detection():
    with sync_playwright() as p:
        browser = p.chromium.launch()
        context = browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        )
        page = context.new_page()

        # Additional headers
        page.set_extra_http_headers({
            'Accept-Language': 'en-US,en;q=0.9',
        })

        page.goto("https://example.com")
        browser.close()
```

#### рзи. **CAPTCHA рж╕ржорж╛ржзрж╛ржи:**
```python
# Manual CAPTCHA solving
def handle_captcha_manually():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)  # headless=False рж░рж╛ржЦрзБржи
        page = browser.new_page()

        page.goto("https://example.com")

        # CAPTCHA ржЖржЫрзЗ ржХрж┐ржирж╛ check ржХрж░рж╛
        if page.locator('.captcha').is_visible():
            print("CAPTCHA detected! Please solve it manually...")

            # User ржПрж░ CAPTCHA solve ржХрж░рж╛рж░ ржЬржирзНржп ржЕржкрзЗржХрзНрж╖рж╛
            page.wait_for_selector('.captcha', state='hidden', timeout=60000)
            print("CAPTCHA solved! Continuing...")

        # Continue scraping
        browser.close()
```

#### рзй. **Rate Limiting Handle ржХрж░рж╛:**
```python
import time
from datetime import datetime, timedelta

class RateLimiter:
    def __init__(self, max_requests=10, time_window=60):
        self.max_requests = max_requests
        self.time_window = time_window
        self.requests = []

    def wait_if_needed(self):
        now = datetime.now()

        # Remove old requests
        self.requests = [req_time for req_time in self.requests
                        if now - req_time < timedelta(seconds=self.time_window)]

        # Check if we need to wait
        if len(self.requests) >= self.max_requests:
            sleep_time = self.time_window - (now - self.requests[0]).seconds
            print(f"Rate limit reached. Waiting {sleep_time} seconds...")
            time.sleep(sleep_time)

        self.requests.append(now)

# Usage
rate_limiter = RateLimiter(max_requests=5, time_window=60)

def scrape_with_rate_limit(urls):
    for url in urls:
        rate_limiter.wait_if_needed()

        # Your scraping code here
        response = requests.get(url)
        print(f"Scraped: {url}")
```

#### рзк. **Dynamic Content Loading:**
```python
def wait_for_dynamic_content():
    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()

        page.goto("https://example.com")

        # Method 1: Wait for specific element
        page.wait_for_selector('.dynamic-content', timeout=10000)

        # Method 2: Wait for network to be idle
        page.wait_for_load_state('networkidle')

        # Method 3: Wait for JavaScript to complete
        page.wait_for_function("document.readyState === 'complete'")

        # Method 4: Custom wait condition
        page.wait_for_function("""
            () => document.querySelectorAll('.item').length > 10
        """)

        browser.close()
```

### ЁЯТб **Performance Optimization ржЯрж┐ржкрж╕:**

#### рзз. **Concurrent Scraping:**
```python
import asyncio
from playwright.async_api import async_playwright

async def scrape_page(url, semaphore):
    async with semaphore:  # Limit concurrent requests
        async with async_playwright() as p:
            browser = await p.chromium.launch()
            page = await browser.new_page()

            await page.goto(url)
            title = await page.title()

            await browser.close()
            return {'url': url, 'title': title}

async def concurrent_scraping():
    urls = [
        "https://example1.com",
        "https://example2.com",
        "https://example3.com",
    ]

    # Maximum 3 concurrent requests
    semaphore = asyncio.Semaphore(3)

    tasks = [scrape_page(url, semaphore) for url in urls]
    results = await asyncio.gather(*tasks)

    return results

# Run async function
results = asyncio.run(concurrent_scraping())
print(results)
```

#### рзи. **Memory Management:**
```python
def memory_efficient_scraping():
    with sync_playwright() as p:
        browser = p.chromium.launch()

        for i in range(100):  # Many pages
            page = browser.new_page()

            try:
                page.goto(f"https://example.com/page/{i}")
                # Scraping logic

            finally:
                page.close()  # Important: Close page to free memory

        browser.close()
```

---

## тЪЦя╕П ржЖржЗржирж┐ ржУ ржирзИрждрж┐ржХ ржмрж┐рж╖ржпрж╝ {#legal-ethics}

### ЁЯУЛ **рж╕рзНржХрзНрж░рзНржпрж╛ржкрж┐ржВ ржПрж░ ржирж┐ржпрж╝ржоржХрж╛ржирзБржи:**

#### тЬЕ **ржпрж╛ ржХрж░рждрзЗ ржкрж╛рж░рзЗржи:**
- **Public Data:** рж╕ржмрж╛рж░ ржЬржирзНржп ржЙржирзНржорзБржХрзНржд рждржерзНржп рж╕ржВржЧрзНрж░рж╣
- **Personal Use:** ржирж┐ржЬрзЗрж░ ржЧржмрзЗрж╖ржгрж╛ ржмрж╛ рж╢рзЗржЦрж╛рж░ ржЬржирзНржп
- **robots.txt ржорзЗржирзЗ ржЪрж▓рж╛:** рж╕рж╛ржЗржЯрзЗрж░ ржирж┐ржпрж╝ржо ржЕржирзБрж╕рж░ржг
- **Rate Limiting:** рж╕рж╛ржЗржЯрзЗрж░ ржЙржкрж░ ржЪрж╛ржк ржХржо ржжрзЗржУржпрж╝рж╛

#### тЭМ **ржпрж╛ ржХрж░ржмрзЗржи ржирж╛:**
- **Copyright Content:** ржХржкрж┐рж░рж╛ржЗржЯ рж╕рзБрж░ржХрзНрж╖рж┐ржд ржХржирзНржЯрзЗржирзНржЯ ржЪрзБрж░рж┐
- **Personal Information:** ржмрзНржпржХрзНрждрж┐ржЧржд рждржерзНржп рж╕ржВржЧрзНрж░рж╣
- **Commercial Misuse:** ржЕржирзБржорждрж┐ ржЫрж╛ржбрж╝рж╛ ржмрзНржпржмрж╕рж╛ржпрж╝рж┐ржХ ржмрзНржпржмрж╣рж╛рж░
- **Server Overload:** рж╕рж╛ржЗржЯ ржХрзНрж░рзНржпрж╛рж╢ ржХрж░рж╛ржирзЛ

### ЁЯдЦ **robots.txt ржЪрзЗржХ ржХрж░рж╛:**
```python
import requests
from urllib.robotparser import RobotFileParser

def check_robots_txt(url, user_agent='*'):
    """
    Check if scraping is allowed according to robots.txt
    """
    try:
        robots_url = f"{url.rstrip('/')}/robots.txt"

        rp = RobotFileParser()
        rp.set_url(robots_url)
        rp.read()

        # Check if URL can be fetched
        can_fetch = rp.can_fetch(user_agent, url)

        print(f"Can scrape {url}: {can_fetch}")

        # Get crawl delay
        crawl_delay = rp.crawl_delay(user_agent)
        if crawl_delay:
            print(f"Recommended delay: {crawl_delay} seconds")

        return can_fetch, crawl_delay

    except Exception as e:
        print(f"Error checking robots.txt: {e}")
        return True, None  # If can't check, assume allowed

# Usage
can_scrape, delay = check_robots_txt("https://example.com")
if can_scrape:
    print("Scraping is allowed!")
else:
    print("Scraping is not allowed by robots.txt")
```

### ЁЯУЬ **Terms of Service ржЪрзЗржХ ржХрж░рж╛:**
```python
def ethical_scraping_guidelines():
    guidelines = """
    ЁЯФН ржирзИрждрж┐ржХ рж╕рзНржХрзНрж░рзНржпрж╛ржкрж┐ржВ ржЧрж╛ржЗржбрж▓рж╛ржЗржи:

    1. ЁЯУЦ Terms of Service ржкржбрж╝рзБржи
    2. ЁЯдЦ robots.txt ржорзЗржирзЗ ржЪрж▓рзБржи
    3. тП▒я╕П Request ржПрж░ ржоржзрзНржпрзЗ delay рж░рж╛ржЦрзБржи
    4. ЁЯФТ Personal data рж╕ржВржЧрзНрж░рж╣ ржХрж░ржмрзЗржи ржирж╛
    5. ЁЯТ░ Commercial use ржПрж░ ржЖржЧрзЗ permission ржирж┐ржи
    6. ЁЯУК Data ржПрж░ source credit ржжрж┐ржи
    7. ЁЯФД Reasonable frequency рждрзЗ scrape ржХрж░рзБржи
    8. ЁЯЪл Site ржПрж░ functionality ржХрзНрж╖рждрж┐ ржХрж░ржмрзЗржи ржирж╛
    """
    print(guidelines)

ethical_scraping_guidelines()
```

---

## ЁЯПЧя╕П ржкрзНрж░ржЬрзЗржХрзНржЯ рж╕рзНржЯрзНрж░рж╛ржХржЪрж╛рж░ ржУ ржмрзЗрж╕рзНржЯ ржкрзНрж░рзНржпрж╛ржХржЯрж┐рж╕

### ЁЯУБ **Recommended Project Structure:**
```
web_scraping_project/
тФВ
тФЬтФАтФА src/
тФВ   тФЬтФАтФА __init__.py
тФВ   тФЬтФАтФА scrapers/
тФВ   тФВ   тФЬтФАтФА __init__.py
тФВ   тФВ   тФЬтФАтФА base_scraper.py
тФВ   тФВ   тФЬтФАтФА ecommerce_scraper.py
тФВ   тФВ   тФФтФАтФА news_scraper.py
тФВ   тФЬтФАтФА utils/
тФВ   тФВ   тФЬтФАтФА __init__.py
тФВ   тФВ   тФЬтФАтФА helpers.py
тФВ   тФВ   тФФтФАтФА data_processor.py
тФВ   тФФтФАтФА config/
тФВ       тФЬтФАтФА __init__.py
тФВ       тФФтФАтФА settings.py
тФВ
тФЬтФАтФА data/
тФВ   тФЬтФАтФА raw/
тФВ   тФЬтФАтФА processed/
тФВ   тФФтФАтФА exports/
тФВ
тФЬтФАтФА tests/
тФВ   тФЬтФАтФА __init__.py
тФВ   тФФтФАтФА test_scrapers.py
тФВ
тФЬтФАтФА logs/
тФЬтФАтФА requirements.txt
тФЬтФАтФА README.md
тФФтФАтФА main.py
```

### ЁЯФз **Base Scraper Class:**
```python
# src/scrapers/base_scraper.py
import logging
import time
import random
from abc import ABC, abstractmethod
from typing import List, Dict, Any

class BaseScraper(ABC):
    def __init__(self, base_url: str, delay_range: tuple = (1, 3)):
        self.base_url = base_url
        self.delay_range = delay_range
        self.session = None
        self.setup_logging()

    def setup_logging(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('logs/scraper.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(self.__class__.__name__)

    def random_delay(self):
        """Random delay between requests"""
        delay = random.uniform(*self.delay_range)
        time.sleep(delay)

    @abstractmethod
    def scrape(self) -> List[Dict[str, Any]]:
        """Main scraping method - must be implemented by subclasses"""
        pass

    def save_data(self, data: List[Dict], filename: str):
        """Save scraped data to file"""
        import json
        with open(f'data/raw/{filename}', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        self.logger.info(f"Data saved to {filename}")

# Example implementation
class EcommerceScraper(BaseScraper):
    def scrape(self) -> List[Dict[str, Any]]:
        products = []

        # Your scraping logic here
        self.logger.info("Starting e-commerce scraping...")

        # Example data
        products.append({
            'name': 'Sample Product',
            'price': 'рзлрзжрзж ржЯрж╛ржХрж╛',
            'rating': 4.5
        })

        return products
```

### тЪЩя╕П **Configuration Management:**
```python
# src/config/settings.py
import os
from dataclasses import dataclass
from typing import List

@dataclass
class ScrapingConfig:
    # Browser settings
    headless: bool = True
    browser_timeout: int = 30000

    # Request settings
    max_retries: int = 3
    delay_range: tuple = (1, 3)
    concurrent_requests: int = 5

    # Data settings
    output_format: str = 'json'  # json, csv, xlsx
    data_dir: str = 'data'

    # Logging
    log_level: str = 'INFO'
    log_file: str = 'logs/scraper.log'

    # User agents
    user_agents: List[str] = None

    def __post_init__(self):
        if self.user_agents is None:
            self.user_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
            ]

# Load config from environment
def load_config():
    return ScrapingConfig(
        headless=os.getenv('HEADLESS', 'true').lower() == 'true',
        max_retries=int(os.getenv('MAX_RETRIES', '3')),
        concurrent_requests=int(os.getenv('CONCURRENT_REQUESTS', '5'))
    )
```

### ЁЯзк **Testing:**
```python
# tests/test_scrapers.py
import unittest
from unittest.mock import Mock, patch
from src.scrapers.ecommerce_scraper import EcommerceScraper

class TestEcommerceScraper(unittest.TestCase):
    def setUp(self):
        self.scraper = EcommerceScraper("https://example.com")

    def test_scraper_initialization(self):
        self.assertEqual(self.scraper.base_url, "https://example.com")
        self.assertIsNotNone(self.scraper.logger)

    @patch('requests.get')
    def test_scrape_method(self, mock_get):
        # Mock response
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.content = "<html><body>Test</body></html>"
        mock_get.return_value = mock_response

        # Test scraping
        result = self.scraper.scrape()
        self.assertIsInstance(result, list)

    def test_random_delay(self):
        import time
        start_time = time.time()
        self.scraper.random_delay()
        end_time = time.time()

        # Check if delay was applied
        self.assertGreater(end_time - start_time, 0.5)

if __name__ == '__main__':
    unittest.main()
```

---

## ЁЯМЯ рж░рж┐ржпрж╝рзЗрж▓-ржУржпрж╝рж╛рж░рзНрж▓рзНржб ржкрзНрж░ржЬрзЗржХрзНржЯ ржЙржжрж╛рж╣рж░ржг

### ЁЯЫТ **E-commerce Price Monitor:**
```python
# main.py - E-commerce price monitoring system
import schedule
import time
from datetime import datetime
from src.scrapers.ecommerce_scraper import EcommerceScraper
from src.utils.data_processor import DataProcessor
from src.utils.helpers import send_notification

class PriceMonitor:
    def __init__(self):
        self.scraper = EcommerceScraper("https://example-shop.com")
        self.processor = DataProcessor()
        self.target_products = [
            {'name': 'iPhone 15', 'target_price': 80000},
            {'name': 'Samsung Galaxy S24', 'target_price': 70000}
        ]

    def monitor_prices(self):
        print(f"ЁЯФН Price monitoring started at {datetime.now()}")

        # Scrape current prices
        current_data = self.scraper.scrape()

        # Process and compare with targets
        for product in current_data:
            for target in self.target_products:
                if target['name'].lower() in product['name'].lower():
                    current_price = self.extract_price(product['price'])

                    if current_price <= target['target_price']:
                        message = f"ЁЯОЙ Price Alert! {product['name']} is now {current_price} ржЯрж╛ржХрж╛"
                        send_notification(message)
                        print(message)

        # Save data with timestamp
        filename = f"price_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        self.scraper.save_data(current_data, filename)

    def extract_price(self, price_text):
        import re
        # Extract numeric price from text like "рзлрзж,рзжрзжрзж ржЯрж╛ржХрж╛"
        numbers = re.findall(r'[\d,]+', price_text)
        if numbers:
            return int(numbers[0].replace(',', ''))
        return 0

    def start_monitoring(self):
        # Schedule monitoring every hour
        schedule.every().hour.do(self.monitor_prices)

        print("ЁЯУК Price monitoring system started!")
        print("Press Ctrl+C to stop...")

        try:
            while True:
                schedule.run_pending()
                time.sleep(60)
        except KeyboardInterrupt:
            print("\nЁЯЫС Monitoring stopped!")

if __name__ == "__main__":
    monitor = PriceMonitor()
    monitor.start_monitoring()
```

### ЁЯУ░ **News Aggregator:**
```python
# news_aggregator.py
from playwright.sync_api import sync_playwright
import json
from datetime import datetime

class NewsAggregator:
    def __init__(self):
        self.news_sources = [
            {
                'name': 'Prothom Alo',
                'url': 'https://www.prothomalo.com',
                'title_selector': '.story-card__title',
                'link_selector': '.story-card__title a'
            },
            {
                'name': 'The Daily Star',
                'url': 'https://www.thedailystar.net',
                'title_selector': '.title',
                'link_selector': '.title a'
            }
        ]

    def scrape_news_source(self, source):
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()

            try:
                page.goto(source['url'])
                page.wait_for_load_state('networkidle')

                # Get all news titles and links
                titles = page.locator(source['title_selector']).all()
                links = page.locator(source['link_selector']).all()

                news_items = []
                for i, title in enumerate(titles[:10]):  # Top 10 news
                    try:
                        title_text = title.text_content().strip()
                        link_href = links[i].get_attribute('href') if i < len(links) else ''

                        # Make absolute URL
                        if link_href.startswith('/'):
                            link_href = source['url'] + link_href

                        news_items.append({
                            'title': title_text,
                            'link': link_href,
                            'source': source['name'],
                            'scraped_at': datetime.now().isoformat()
                        })
                    except Exception as e:
                        print(f"Error processing item {i}: {e}")
                        continue

                return news_items

            except Exception as e:
                print(f"Error scraping {source['name']}: {e}")
                return []

            finally:
                browser.close()

    def aggregate_all_news(self):
        all_news = []

        for source in self.news_sources:
            print(f"ЁЯУ░ Scraping {source['name']}...")
            news_items = self.scrape_news_source(source)
            all_news.extend(news_items)

            # Delay between sources
            import time
            time.sleep(2)

        # Save aggregated news
        filename = f"news_aggregated_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(f'data/raw/{filename}', 'w', encoding='utf-8') as f:
            json.dump(all_news, f, ensure_ascii=False, indent=2)

        print(f"тЬЕ Total {len(all_news)} news items collected!")
        return all_news

# Usage
if __name__ == "__main__":
    aggregator = NewsAggregator()
    news = aggregator.aggregate_all_news()

    # Print top 5 news
    for i, item in enumerate(news[:5]):
        print(f"{i+1}. {item['title']} - {item['source']}")
```

### ЁЯПв **Job Listings Scraper:**
```python
# job_scraper.py
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime

class JobScraper:
    def __init__(self):
        self.base_url = "https://jobs.example.com"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

    def scrape_jobs(self, keyword="python developer", location="dhaka"):
        jobs = []
        page = 1

        while page <= 5:  # Scrape first 5 pages
            print(f"ЁЯФН Scraping page {page}...")

            params = {
                'q': keyword,
                'l': location,
                'p': page
            }

            response = requests.get(f"{self.base_url}/search",
                                  params=params, headers=self.headers)

            if response.status_code != 200:
                print(f"Error: {response.status_code}")
                break

            soup = BeautifulSoup(response.content, 'html.parser')
            job_cards = soup.find_all('div', class_='job-card')

            if not job_cards:
                print("No more jobs found!")
                break

            for card in job_cards:
                try:
                    job_data = {
                        'title': card.find('h3', class_='job-title').text.strip(),
                        'company': card.find('span', class_='company-name').text.strip(),
                        'location': card.find('span', class_='job-location').text.strip(),
                        'salary': self.extract_salary(card),
                        'posted_date': card.find('span', class_='posted-date').text.strip(),
                        'job_url': self.base_url + card.find('a')['href'],
                        'scraped_at': datetime.now().isoformat()
                    }
                    jobs.append(job_data)

                except Exception as e:
                    print(f"Error parsing job card: {e}")
                    continue

            page += 1
            import time
            time.sleep(2)  # Be respectful

        return jobs

    def extract_salary(self, card):
        salary_elem = card.find('span', class_='salary')
        if salary_elem:
            return salary_elem.text.strip()
        return "Not specified"

    def save_to_excel(self, jobs, filename):
        df = pd.DataFrame(jobs)

        # Data cleaning
        df['salary_numeric'] = df['salary'].str.extract('(\d+)').astype(float)

        # Save to Excel with multiple sheets
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='All Jobs', index=False)

            # High salary jobs (if salary info available)
            high_salary = df[df['salary_numeric'] > 50000]
            if not high_salary.empty:
                high_salary.to_excel(writer, sheet_name='High Salary', index=False)

            # Jobs by company
            company_summary = df.groupby('company').size().reset_index(name='job_count')
            company_summary.to_excel(writer, sheet_name='Company Summary', index=False)

        print(f"ЁЯУК Data saved to {filename}")

# Usage
if __name__ == "__main__":
    scraper = JobScraper()

    # Scrape Python developer jobs in Dhaka
    jobs = scraper.scrape_jobs("python developer", "dhaka")

    if jobs:
        filename = f"jobs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        scraper.save_to_excel(jobs, filename)

        print(f"тЬЕ Found {len(jobs)} jobs!")

        # Show top 5 jobs
        for i, job in enumerate(jobs[:5]):
            print(f"{i+1}. {job['title']} at {job['company']} - {job['location']}")
    else:
        print("тЭМ No jobs found!")
```

---

## ЁЯОУ рж╢рзЗржЦрж╛рж░ ржкрже ржУ ржкрж░ржмрж░рзНрждрзА ржзрж╛ржк

### ЁЯУЪ **рж╢рж┐ржХрзНрж╖рж╛ржиржмрж┐рж╕ ржерзЗржХрзЗ ржПржХрзНрж╕ржкрж╛рж░рзНржЯ рж╣ржУржпрж╝рж╛рж░ рж░рзЛржбржорзНржпрж╛ржк:**

#### ЁЯеЙ **Level 1: Beginner (рзз-рзи ржорж╛рж╕)**
- [ ] HTML/CSS ржмрзЗрж╕рж┐ржХ рж╢рж┐ржЦрзБржи
- [ ] Python ржмрзЗрж╕рж┐ржХ рж╢рж┐ржЦрзБржи
- [ ] requests + BeautifulSoup ржжрж┐ржпрж╝рзЗ рж╕рж┐ржорзНржкрж▓ рж╕рзНржХрзНрж░рзНржпрж╛ржкрж┐ржВ
- [ ] CSV ржлрж╛ржЗрж▓рзЗ ржбрзЗржЯрж╛ рж╕рзЗржн ржХрж░рж╛
- [ ] DevTools ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛

#### ЁЯеИ **Level 2: Intermediate (рзи-рзк ржорж╛рж╕)**
- [ ] Playwright/Selenium рж╢рж┐ржЦрзБржи
- [ ] JavaScript rendering handle ржХрж░рж╛
- [ ] Form submission ржУ login
- [ ] Database ржП ржбрзЗржЯрж╛ рж╕рзЗржн ржХрж░рж╛
- [ ] Error handling ржУ retry logic
- [ ] Basic data analysis with Pandas

#### ЁЯеЗ **Level 3: Advanced (рзк-рзм ржорж╛рж╕)**
- [ ] Async/concurrent scraping
- [ ] API reverse engineering
- [ ] CAPTCHA solving
- [ ] Proxy rotation
- [ ] Distributed scraping
- [ ] Machine learning for data extraction

#### ЁЯПЖ **Level 4: Expert (рзм+ ржорж╛рж╕)**
- [ ] Custom browser automation
- [ ] Anti-detection techniques
- [ ] Large-scale data pipelines
- [ ] Real-time monitoring systems
- [ ] Cloud deployment (AWS/GCP)
- [ ] Contributing to open-source projects

### ЁЯФЧ **ржжрж░ржХрж╛рж░рзА рж░рж┐рж╕рзЛрж░рзНрж╕:**

#### ЁЯУЦ **ржмржЗ:**
- "Web Scraping with Python" by Ryan Mitchell
- "Automate the Boring Stuff with Python" by Al Sweigart

#### ЁЯМР **ржУржпрж╝рзЗржмрж╕рж╛ржЗржЯ:**
- [Scrapy Documentation](https://scrapy.org/)
- [Playwright Documentation](https://playwright.dev/)
- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/)

#### ЁЯОе **YouTube Channels:**
- Corey Schafer (Python tutorials)
- Tech With Tim (Web scraping tutorials)

#### ЁЯТм **ржХржорж┐ржЙржирж┐ржЯрж┐:**
- Reddit: r/webscraping
- Stack Overflow
- GitHub (open source projects)

---

## ЁЯОп рж╕ржорж╛ржкржирзА

ржПржЗ ржЧрж╛ржЗржбрзЗ ржЖржорж░рж╛ ржУржпрж╝рзЗржм рж╕рзНржХрзНрж░рзНржпрж╛ржкрж┐ржВ ржПрж░ A-Z рж╕ржм ржХрж┐ржЫрзБ ржХржнрж╛рж░ ржХрж░рзЗржЫрж┐ред ржоржирзЗ рж░рж╛ржЦржмрзЗржи:

### тЬЕ **ржорзВрж▓ ржмрж┐рж╖ржпрж╝ржЧрзБрж▓рзЛ:**
1. **ржзрзИрж░рзНржп рж░рж╛ржЦрзБржи:** рж╕рзНржХрзНрж░рзНржпрж╛ржкрж┐ржВ рж╢рзЗржЦрж╛ ржПржХржЯрж┐ ржкрзНрж░ржХрзНрж░рж┐ржпрж╝рж╛
2. **ржирзИрждрж┐ржХрждрж╛ ржорзЗржирзЗ ржЪрж▓рзБржи:** robots.txt ржУ ToS respect ржХрж░рзБржи
3. **ржкрзНрж░рзНржпрж╛ржХржЯрж┐рж╕ ржХрж░рзБржи:** ржЫрзЛржЯ ржкрзНрж░ржЬрзЗржХрзНржЯ ржжрж┐ржпрж╝рзЗ рж╢рзБрж░рзБ ржХрж░рзБржи
4. **ржЖржкржбрзЗржЯ ржерж╛ржХрзБржи:** ржУржпрж╝рзЗржмрж╕рж╛ржЗржЯ ржкрж░рж┐ржмрж░рзНрждржи рж╣ржпрж╝, ржХрзЛржбржУ ржЖржкржбрзЗржЯ ржХрж░рзБржи

### ЁЯЪА **ржкрж░ржмрж░рзНрждрзА ржкржжржХрзНрж╖рзЗржк:**
1. ржПржЗ ржЧрж╛ржЗржбрзЗрж░ ржЙржжрж╛рж╣рж░ржгржЧрзБрж▓рзЛ ржЪрж╛рж▓рж┐ржпрж╝рзЗ ржжрзЗржЦрзБржи
2. ржирж┐ржЬрзЗрж░ ржПржХржЯрж┐ ржкрзНрж░ржЬрзЗржХрзНржЯ рж╢рзБрж░рзБ ржХрж░рзБржи
3. GitHub ржП ржХрзЛржб рж╢рзЗржпрж╝рж╛рж░ ржХрж░рзБржи
4. ржХржорж┐ржЙржирж┐ржЯрж┐рждрзЗ ржпрзЛржЧ ржжрж┐ржи

### ЁЯТб **ржоржирзЗ рж░рж╛ржЦрзБржи:**
> "The best way to learn web scraping is by doing it!"

**Happy Scraping! ЁЯХ╖я╕ПЁЯЗзЁЯЗй**

---

*ржПржЗ ржбржХрзБржорзЗржирзНржЯрзЗрж╢ржи ржирж┐ржпрж╝ржорж┐ржд ржЖржкржбрзЗржЯ ржХрж░рж╛ рж╣ржмрзЗред ржирждрзБржи ржЯрзБрж▓рж╕ ржУ ржЯрзЗржХржирж┐ржХ ржпрзЛржЧ ржХрж░рж╛ рж╣ржмрзЗред*
```
```
```
```
