# üé≠ Playwright ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶ì‡¶Ø‡¶º‡ßá‡¶¨ ‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶™‡¶ø‡¶Ç - A to Z ‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶ó‡¶æ‡¶á‡¶° üáßüá©

## üìö ‡¶∏‡ßÇ‡¶ö‡¶ø‡¶™‡¶§‡ßç‡¶∞
1. [Playwright ‡¶ï‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶ï‡ßá‡¶®?](#what-is-playwright)
2. [‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤‡ßá‡¶∂‡¶® ‡¶ì ‡¶∏‡ßá‡¶ü‡¶Ü‡¶™](#installation-setup)
3. [‡¶°‡ßá‡¶≠‡ßá‡¶≤‡¶™‡¶æ‡¶∞ ‡¶ü‡ßÅ‡¶≤‡¶∏ (DevTools) ‡¶Æ‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡¶ø](#devtools-mastery)
4. [‡¶®‡ßá‡¶ü‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞‡ßç‡¶ï ‡¶ü‡ßç‡¶Ø‡¶æ‡¶¨ ‡¶Æ‡¶®‡¶ø‡¶ü‡¶∞‡¶ø‡¶Ç](#network-monitoring)
5. [HTML/CSS ‡¶∏‡¶ø‡¶≤‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶ï‡ßç‡¶∏‡¶™‡¶æ‡¶∞‡ßç‡¶ü](#selectors-expert)
6. [Playwright ‡¶¨‡ßá‡¶∏‡¶ø‡¶ï - Browser ‡¶ì Page](#playwright-basics)
7. [Element Interaction - Click, Type, Fill](#element-interaction)
8. [‡¶Ö‡ßç‡¶Ø‡¶æ‡¶°‡¶≠‡¶æ‡¶®‡ßç‡¶∏ ‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶™‡¶ø‡¶Ç ‡¶ü‡ßá‡¶ï‡¶®‡¶ø‡¶ï](#advanced-scraping)
9. [JavaScript Execution ‡¶ì Evaluation](#javascript-execution)
10. [Request/Response Interception](#request-interception)
11. [Mobile ‡¶ì Multi-Browser Testing](#mobile-multi-browser)
12. [Performance ‡¶ì Optimization](#performance-optimization)
13. [Error Handling ‡¶ì Debugging](#error-handling)
14. [Real-World ‡¶™‡ßç‡¶∞‡¶ú‡ßá‡¶ï‡ßç‡¶ü](#real-world-projects)
15. [Best Practices ‡¶ì Security](#best-practices)

---

## üé≠ Playwright ‡¶ï‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶ï‡ßá‡¶®? {#what-is-playwright}

### ü§î Playwright ‡¶ï‡¶ø?
**Playwright** ‡¶π‡¶≤‡ßã Microsoft ‡¶è‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∂‡¶ï‡ßç‡¶§‡¶ø‡¶∂‡¶æ‡¶≤‡ßÄ browser automation library ‡¶Ø‡¶æ ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ü‡¶™‡¶®‡¶ø:
- **Real Browser Control:** Chrome, Firefox, Safari ‡¶®‡¶ø‡¶Ø‡¶º‡¶®‡ßç‡¶§‡ßç‡¶∞‡¶£ ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®
- **JavaScript Rendering:** Dynamic content load ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®
- **User Interaction:** Click, type, scroll ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®
- **Mobile Testing:** Mobile device emulate ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®

### üöÄ ‡¶ï‡ßá‡¶® Playwright ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶¨‡ßá‡¶®?

#### ‚úÖ **Playwright ‡¶è‡¶∞ ‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:**
- **Fast & Reliable:** ‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø tools ‡¶•‡ßá‡¶ï‡ßá ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§
- **Multi-Browser:** Chrome, Firefox, Safari support
- **Mobile Support:** iOS/Android emulation
- **Network Control:** Request/Response intercept ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®
- **Screenshot/PDF:** Page capture ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®
- **Headless/Headed:** ‡¶¶‡ßÅ‡¶ü‡ßã mode ‡¶è ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá

#### ‚ùå **‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø Tools ‡¶è‡¶∞ ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ:**
- **Requests + BeautifulSoup:** JavaScript render ‡¶ï‡¶∞‡ßá ‡¶®‡¶æ
- **Selenium:** ‡¶ß‡ßÄ‡¶∞ ‡¶è‡¶¨‡¶Ç unstable
- **Puppeteer:** ‡¶∂‡ßÅ‡¶ß‡ßÅ Chrome support

### üéØ ‡¶ï‡¶ñ‡¶® Playwright ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶¨‡ßá‡¶®?
- ‚úÖ **Dynamic Content:** JavaScript ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá load ‡¶π‡¶ì‡¶Ø‡¶º‡¶æ content
- ‚úÖ **User Interaction:** Form fill, button click ‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶ú‡¶®
- ‚úÖ **SPA (Single Page Apps):** React, Vue, Angular sites
- ‚úÖ **Authentication:** Login ‡¶ï‡¶∞‡ßá data ‡¶®‡¶ø‡¶§‡ßá ‡¶π‡¶¨‡ßá
- ‚úÖ **Complex Navigation:** Multi-step process

---

## üõ†Ô∏è ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤‡ßá‡¶∂‡¶® ‡¶ì ‡¶∏‡ßá‡¶ü‡¶Ü‡¶™ {#installation-setup}

### üìã ‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶ú‡¶®‡ßÄ‡¶Ø‡¶º ‡¶∏‡¶´‡¶ü‡¶ì‡¶Ø‡¶º‡ßç‡¶Ø‡¶æ‡¶∞:
1. **Python 3.8+** (‡¶Ö‡¶¨‡¶∂‡ßç‡¶Ø‡¶á ‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶ú‡¶®)
2. **VS Code** ‡¶¨‡¶æ ‡¶Ø‡ßá‡¶ï‡ßã‡¶®‡ßã code editor
3. **Terminal/Command Prompt**

### üêç Playwright ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®:
```bash
# Playwright Python package install
pip install playwright

# Browser binaries install (‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£!)
playwright install

# Specific browser install ‡¶ï‡¶∞‡¶§‡ßá ‡¶ö‡¶æ‡¶á‡¶≤‡ßá
playwright install chromium
playwright install firefox
playwright install webkit
```

### ‚úÖ ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤‡ßá‡¶∂‡¶® ‡¶ü‡ßá‡¶∏‡ßç‡¶ü:
```python
# test_installation.py
from playwright.sync_api import sync_playwright

def test_playwright():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()
        page.goto("https://example.com")
        print(f"Page title: {page.title()}")
        browser.close()
        print("‚úÖ Playwright successfully installed!")

if __name__ == "__main__":
    test_playwright()
```

### üöÄ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ Playwright ‡¶™‡ßç‡¶∞‡ßã‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ:
```python
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    # Browser launch ‡¶ï‡¶∞‡ßÅ‡¶®
    browser = p.chromium.launch(headless=False)  # headless=True ‡¶ï‡¶∞‡¶≤‡ßá browser ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡¶¨‡ßá ‡¶®‡¶æ

    # ‡¶®‡¶§‡ßÅ‡¶® page ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®
    page = browser.new_page()

    # Website ‡¶è ‡¶Ø‡¶æ‡¶®
    page.goto("https://example.com")

    # Page title print ‡¶ï‡¶∞‡ßÅ‡¶®
    print(f"Title: {page.title()}")

    # Browser ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßÅ‡¶®
    browser.close()
```

---

## üîß ‡¶°‡ßá‡¶≠‡ßá‡¶≤‡¶™‡¶æ‡¶∞ ‡¶ü‡ßÅ‡¶≤‡¶∏ (DevTools) ‡¶Æ‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡¶ø {#devtools-mastery}

### üñ±Ô∏è DevTools ‡¶ñ‡ßã‡¶≤‡¶æ‡¶∞ ‡¶â‡¶™‡¶æ‡¶Ø‡¶º:
- **Windows/Linux:** `F12` ‡¶¨‡¶æ `Ctrl+Shift+I`
- **Mac:** `Cmd+Option+I`
- **Right Click:** "Inspect Element"
- **Playwright ‡¶•‡ßá‡¶ï‡ßá:** `args=["--auto-open-devtools-for-tabs"]`

### üì± DevTools ‡¶è‡¶∞ ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶ü‡ßç‡¶Ø‡¶æ‡¶¨‡¶∏‡¶Æ‡ßÇ‡¶π:

#### 1Ô∏è‚É£ **Elements Tab** (HTML/CSS Analysis)
```html
<!-- ‡¶è‡¶≠‡¶æ‡¶¨‡ßá HTML structure ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶™‡¶æ‡¶¨‡ßá‡¶® -->
<div class="product" data-id="123">
    <h2 class="title">‡¶™‡¶£‡ßç‡¶Ø‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ</h2>
    <span class="price" data-currency="BDT">‡ß´‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ</span>
    <button class="add-to-cart" onclick="addToCart(123)">Add to Cart</button>
</div>
```

**üéØ Advanced Element Inspection:**
- **Element select:** `Ctrl+Shift+C` (Windows) / `Cmd+Shift+C` (Mac)
- **CSS properties:** Right panel ‡¶è "Styles", "Computed", "Layout"
- **Selector copy:** Right click ‚Üí Copy ‚Üí Copy selector/XPath/JS path
- **Event listeners:** "Event Listeners" tab ‡¶è click events ‡¶¶‡ßá‡¶ñ‡ßÅ‡¶®
- **Accessibility:** "Accessibility" tab ‡¶è screen reader info

#### 2Ô∏è‚É£ **Console Tab** (JavaScript Testing Ground)
```javascript
// Console ‡¶è ‡¶è‡¶á advanced ‡¶ï‡¶Æ‡¶æ‡¶®‡ßç‡¶°‡¶ó‡ßÅ‡¶≤‡ßã ‡¶ö‡¶æ‡¶≤‡¶ø‡¶Ø‡¶º‡ßá ‡¶¶‡ßá‡¶ñ‡ßÅ‡¶®

// Basic DOM queries
document.title                           // ‡¶™‡ßá‡¶ú‡ßá‡¶∞ ‡¶ü‡¶æ‡¶á‡¶ü‡ßá‡¶≤
document.querySelector('h1')             // ‡¶™‡ßç‡¶∞‡¶•‡¶Æ h1 element
document.querySelectorAll('.price')      // ‡¶∏‡¶¨ price class elements
document.getElementById('main-content')   // ID ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá element

// Advanced queries
$('h1')                                  // jQuery-style selector (Chrome DevTools)
$$('.product')                           // ‡¶∏‡¶¨ .product elements (Chrome DevTools)
$x('//div[@class="product"]')            // XPath selector

// Element properties
let element = document.querySelector('.price');
element.textContent                      // Text content
element.innerHTML                        // HTML content
element.getAttribute('data-currency')    // Attribute value
element.style.color = 'red'             // Style change

// Page manipulation
window.scrollTo(0, document.body.scrollHeight)  // Scroll to bottom
localStorage.getItem('user_data')        // Local storage data
sessionStorage.setItem('test', 'value')  // Session storage
document.cookie                          // All cookies

// Network and performance
performance.now()                        // Current timestamp
navigator.userAgent                      // Browser info
window.location.href                     // Current URL
```

#### 3Ô∏è‚É£ **Sources Tab** (JavaScript Debugging)
- **File explorer:** ‡¶∏‡¶¨ JavaScript, CSS files
- **Breakpoints:** Code execution ‡¶•‡¶æ‡¶Æ‡¶æ‡¶®‡ßã‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
- **Watch expressions:** Variable values monitor ‡¶ï‡¶∞‡¶æ
- **Call stack:** Function call hierarchy
- **Scope:** Current scope ‡¶è‡¶∞ variables

**üîç Debugging ‡¶ü‡¶ø‡¶™‡¶∏:**
```javascript
// Console ‡¶è debugging commands
debugger;                    // Code ‡¶è breakpoint set ‡¶ï‡¶∞‡¶æ
console.log('Debug info');   // Simple logging
console.table(data);         // Table format ‡¶è data
console.time('operation');   // Performance timing start
console.timeEnd('operation'); // Performance timing end
```

---

## üì° ‡¶®‡ßá‡¶ü‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞‡ßç‡¶ï ‡¶ü‡ßç‡¶Ø‡¶æ‡¶¨ ‡¶Æ‡¶®‡¶ø‡¶ü‡¶∞‡¶ø‡¶Ç {#network-monitoring}

### üåê Network Tab ‡¶ï‡ßá‡¶® ‡¶Ö‡¶§‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£?
Network Tab ‡¶π‡¶≤‡ßã web scraping ‡¶è‡¶∞ **‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶∂‡¶ï‡ßç‡¶§‡¶ø‡¶∂‡¶æ‡¶≤‡ßÄ ‡¶ü‡ßÅ‡¶≤**‡•§ ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶Ü‡¶™‡¶®‡¶ø ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶™‡¶æ‡¶¨‡ßá‡¶®:
- üîç **API Endpoints:** Hidden API calls
- üì° **AJAX Requests:** Dynamic content loading
- üç™ **Cookies & Headers:** Authentication info
- üìä **Response Data:** Raw JSON/XML data
- ‚è±Ô∏è **Timing Info:** Performance metrics

### üîç Network Tab Advanced ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞:

#### ‡¶ß‡¶æ‡¶™ ‡ßß: Network Tab Setup
```javascript
// DevTools ‡¶ñ‡ßã‡¶≤‡¶æ‡¶∞ ‡¶™‡¶∞ Console ‡¶è ‡¶è‡¶á command ‡¶ö‡¶æ‡¶≤‡¶æ‡¶®
// Network requests clear ‡¶ï‡¶∞‡¶§‡ßá
console.clear();

// ‡¶Ö‡¶•‡¶¨‡¶æ Playwright ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá auto-open
```

```python
# Playwright ‡¶è DevTools auto-open
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch(
        headless=False,
        args=["--auto-open-devtools-for-tabs"]
    )
    page = browser.new_page()
    page.goto("https://example.com")
```

#### ‡¶ß‡¶æ‡¶™ ‡ß®: Request Types ‡¶ì Filtering
```
üéØ Advanced Filter Options:
- All: ‡¶∏‡¶¨ requests (default)
- Fetch/XHR: API calls ‡¶è‡¶¨‡¶Ç AJAX requests
- JS: JavaScript files
- CSS: Stylesheet files
- Img: Images (PNG, JPG, SVG)
- Media: Videos, Audio files
- Font: Web fonts (WOFF, TTF)
- Doc: HTML documents
- WS: WebSocket connections
- Manifest: PWA manifest files
```

#### ‡¶ß‡¶æ‡¶™ ‡ß©: Request Analysis (‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞‡¶ø‡¶§)
‡¶è‡¶ï‡¶ü‡¶ø request ‡¶è ‡¶ï‡ßç‡¶≤‡¶ø‡¶ï ‡¶ï‡¶∞‡¶≤‡ßá ‡¶è‡¶á tabs ‡¶™‡¶æ‡¶¨‡ßá‡¶®:

**üìã Headers Tab:**
```
General:
- Request URL: https://api.example.com/products
- Request Method: GET/POST/PUT/DELETE
- Status Code: 200 OK / 404 Not Found
- Remote Address: 192.168.1.1:443

Request Headers:
- User-Agent: Browser information
- Accept: Content types accepted
- Authorization: Bearer token/API key
- Cookie: Session cookies
- Referer: Previous page URL

Response Headers:
- Content-Type: application/json
- Set-Cookie: New cookies
- Access-Control-Allow-Origin: CORS settings
- Cache-Control: Caching rules
```

**üëÅÔ∏è Preview Tab:**
- JSON data ‡¶è‡¶∞ formatted view
- Images ‡¶è‡¶∞ preview
- HTML content ‡¶è‡¶∞ rendered view

**üìÑ Response Tab:**
- Raw response data
- JSON, XML, HTML source code
- Binary data (images, files)

**‚è±Ô∏è Timing Tab:**
```
Request timing breakdown:
- Queueing: Request queue time
- Stalled: Network stack delay
- DNS Lookup: Domain resolution
- Initial Connection: TCP handshake
- SSL: SSL/TLS negotiation
- Request Sent: Upload time
- Waiting (TTFB): Time to First Byte
- Content Download: Download time
```

### üéØ Playwright ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá Network Monitoring:

#### Request/Response Interception:
```python
from playwright.sync_api import sync_playwright

def monitor_network():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()

        # All network requests capture ‡¶ï‡¶∞‡¶æ
        requests_log = []

        def handle_request(request):
            requests_log.append({
                'url': request.url,
                'method': request.method,
                'headers': request.headers,
                'post_data': request.post_data
            })
            print(f"üì§ Request: {request.method} {request.url}")

        def handle_response(response):
            print(f"üì• Response: {response.status} {response.url}")

            # JSON response parse ‡¶ï‡¶∞‡¶æ
            if 'application/json' in response.headers.get('content-type', ''):
                try:
                    json_data = response.json()
                    print(f"üìä JSON Data: {json_data}")
                except:
                    pass

        # Event listeners attach ‡¶ï‡¶∞‡¶æ
        page.on('request', handle_request)
        page.on('response', handle_response)

        # Website navigate ‡¶ï‡¶∞‡¶æ
        page.goto("https://example.com")
        page.wait_for_load_state('networkidle')

        # Captured requests analysis
        print(f"\nüìã Total Requests: {len(requests_log)}")
        for req in requests_log:
            if 'api' in req['url']:
                print(f"üîç API Found: {req['url']}")

        browser.close()

monitor_network()
```

### üí° Real-World Network Analysis Example:
```python
def find_hidden_apis():
    """E-commerce site ‡¶è‡¶∞ hidden API ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ"""

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()

        api_endpoints = []

        def capture_api_calls(response):
            url = response.url

            # API patterns ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ
            api_patterns = ['/api/', '/ajax/', '.json', '/graphql', '/rest/']

            if any(pattern in url for pattern in api_patterns):
                api_info = {
                    'url': url,
                    'status': response.status,
                    'method': response.request.method,
                    'headers': dict(response.headers)
                }

                # Response data capture ‡¶ï‡¶∞‡¶æ
                try:
                    if response.headers.get('content-type', '').startswith('application/json'):
                        api_info['data'] = response.json()
                except:
                    pass

                api_endpoints.append(api_info)
                print(f"üéØ API Found: {url}")

        page.on('response', capture_api_calls)

        # E-commerce site navigate ‡¶ï‡¶∞‡¶æ
        page.goto("https://example-shop.com")

        # Product search ‡¶ï‡¶∞‡¶æ (API calls trigger ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø)
        search_box = page.locator('input[type="search"]')
        if search_box.is_visible():
            search_box.fill('laptop')
            search_box.press('Enter')
            page.wait_for_load_state('networkidle')

        # Results analysis
        print(f"\nüîç Found {len(api_endpoints)} API endpoints:")
        for api in api_endpoints:
            print(f"  üì° {api['method']} {api['url']} - Status: {api['status']}")

        browser.close()
        return api_endpoints

# Function call ‡¶ï‡¶∞‡¶æ
apis = find_hidden_apis()
```

---

## üéØ HTML/CSS ‡¶∏‡¶ø‡¶≤‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶ï‡ßç‡¶∏‡¶™‡¶æ‡¶∞‡ßç‡¶ü {#selectors-expert}

### üìù HTML Structure Deep Dive:
```html
<!DOCTYPE html>
<html lang="bn">
<head>
    <title>E-commerce Product Page</title>
    <meta charset="UTF-8">
</head>
<body>
    <nav class="navbar" id="main-nav">
        <ul class="nav-links">
            <li><a href="/home" class="nav-link active">Home</a></li>
            <li><a href="/products" class="nav-link">Products</a></li>
        </ul>
    </nav>

    <main class="container">
        <section class="product-section" data-category="electronics">
            <h1 id="product-title" class="title main-title">iPhone 15 Pro</h1>
            <div class="product-info">
                <span class="price" data-currency="BDT" data-amount="120000">‡ßß,‡ß®‡ß¶,‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ</span>
                <div class="rating" data-rating="4.5">
                    <span class="stars">‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ</span>
                    <span class="rating-text">(4.5/5)</span>
                </div>
            </div>

            <ul class="features-list">
                <li class="feature-item" data-feature="camera">48MP Camera</li>
                <li class="feature-item" data-feature="storage">256GB Storage</li>
                <li class="feature-item" data-feature="display">6.1" Display</li>
            </ul>

            <form class="purchase-form" id="buy-form">
                <input type="number" name="quantity" value="1" min="1" class="quantity-input">
                <button type="submit" class="btn btn-primary add-to-cart" data-product-id="12345">
                    Add to Cart
                </button>
            </form>
        </section>

        <aside class="sidebar">
            <div class="related-products">
                <h3>Related Products</h3>
                <div class="product-card" data-id="67890">
                    <img src="product2.jpg" alt="iPhone 14" class="product-image">
                    <h4 class="product-name">iPhone 14</h4>
                    <span class="product-price">‡ßØ‡ß´,‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ</span>
                </div>
            </div>
        </aside>
    </main>

    <footer class="footer">
        <p>&copy; 2024 My Shop</p>
    </footer>
</body>
</html>
```

### üéØ CSS Selectors - Complete Guide:

#### 1Ô∏è‚É£ **‡¶¨‡ßá‡¶∏‡¶ø‡¶ï ‡¶∏‡¶ø‡¶≤‡ßá‡¶ï‡ßç‡¶ü‡¶∞:**
```css
/* Element/Tag Selector */
h1                  ‚Üí ‡¶∏‡¶¨ h1 elements
div                 ‚Üí ‡¶∏‡¶¨ div elements
span                ‚Üí ‡¶∏‡¶¨ span elements

/* Class Selector */
.container          ‚Üí class="container"
.product-info       ‚Üí class="product-info"
.btn.btn-primary    ‚Üí class="btn btn-primary" (multiple classes)

/* ID Selector */
#product-title      ‚Üí id="product-title"
#main-nav           ‚Üí id="main-nav"

/* Universal Selector */
*                   ‚Üí ‡¶∏‡¶¨ elements
```

#### 2Ô∏è‚É£ **Attribute Selectors:**
```css
/* Attribute exists */
[data-category]     ‚Üí data-category attribute ‡¶Ü‡¶õ‡ßá
[href]              ‚Üí href attribute ‡¶Ü‡¶õ‡ßá

/* Attribute equals */
[data-currency="BDT"]     ‚Üí data-currency="BDT"
[type="submit"]           ‚Üí type="submit"

/* Attribute contains */
[class*="btn"]            ‚Üí class ‡¶è "btn" ‡¶Ü‡¶õ‡ßá
[data-feature*="camera"]  ‚Üí data-feature ‡¶è "camera" ‡¶Ü‡¶õ‡ßá

/* Attribute starts with */
[class^="product"]        ‚Üí class "product" ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶∂‡ßÅ‡¶∞‡ßÅ
[href^="https"]           ‚Üí href "https" ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶∂‡ßÅ‡¶∞‡ßÅ

/* Attribute ends with */
[class$="primary"]        ‚Üí class "primary" ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶∂‡ßá‡¶∑
[src$=".jpg"]             ‚Üí src ".jpg" ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶∂‡ßá‡¶∑
```

#### 3Ô∏è‚É£ **Combinators (‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡ßá‡¶∂‡¶ï):**
```css
/* Descendant (‡¶¨‡¶Ç‡¶∂‡¶ß‡¶∞) */
.container h1             ‚Üí container ‡¶è‡¶∞ ‡¶≠‡¶ø‡¶§‡¶∞‡ßá‡¶∞ ‡¶Ø‡ßá‡¶ï‡ßã‡¶®‡ßã h1
.product-section span     ‚Üí product-section ‡¶è‡¶∞ ‡¶≠‡¶ø‡¶§‡¶∞‡ßá‡¶∞ ‡¶Ø‡ßá‡¶ï‡ßã‡¶®‡ßã span

/* Child (‡¶∏‡¶®‡ßç‡¶§‡¶æ‡¶®) */
.container > h1           ‚Üí container ‡¶è‡¶∞ direct child h1
.nav-links > li           ‚Üí nav-links ‡¶è‡¶∞ direct child li

/* Adjacent Sibling (‡¶™‡¶æ‡¶∂‡ßá‡¶∞ ‡¶≠‡¶æ‡¶á) */
h1 + div                  ‚Üí h1 ‡¶è‡¶∞ ‡¶†‡¶ø‡¶ï ‡¶™‡¶∞‡ßá‡¶∞ div
.price + .rating          ‚Üí price ‡¶è‡¶∞ ‡¶†‡¶ø‡¶ï ‡¶™‡¶∞‡ßá‡¶∞ rating

/* General Sibling (‡¶∏‡¶¨ ‡¶≠‡¶æ‡¶á) */
h1 ~ div                  ‚Üí h1 ‡¶è‡¶∞ ‡¶™‡¶∞‡ßá‡¶∞ ‡¶∏‡¶¨ div
.price ~ span             ‚Üí price ‡¶è‡¶∞ ‡¶™‡¶∞‡ßá‡¶∞ ‡¶∏‡¶¨ span
```

#### 4Ô∏è‚É£ **Pseudo Selectors:**
```css
/* Structural Pseudo-classes */
li:first-child            ‚Üí ‡¶™‡ßç‡¶∞‡¶•‡¶Æ li
li:last-child             ‚Üí ‡¶∂‡ßá‡¶∑ li
li:nth-child(2)           ‚Üí ‡ß®‡¶Ø‡¶º li
li:nth-child(odd)         ‚Üí ‡¶¨‡¶ø‡¶ú‡ßã‡¶°‡¶º position ‡¶è‡¶∞ li
li:nth-child(even)        ‚Üí ‡¶ú‡ßã‡¶°‡¶º position ‡¶è‡¶∞ li
li:nth-child(3n+1)        ‚Üí ‡ß©n+‡ßß position ‡¶è‡¶∞ li

/* State Pseudo-classes */
a:hover                   ‚Üí mouse hover ‡¶ï‡¶∞‡¶≤‡ßá
input:focus               ‚Üí input field focus ‡¶ï‡¶∞‡¶≤‡ßá
button:disabled           ‚Üí disabled button
input:checked             ‚Üí checked checkbox/radio

/* Content Pseudo-classes */
p:empty                   ‚Üí ‡¶ñ‡¶æ‡¶≤‡¶ø p elements
div:not(.sidebar)         ‚Üí sidebar class ‡¶®‡ßá‡¶á ‡¶è‡¶Æ‡¶® div
```

#### 5Ô∏è‚É£ **Advanced Selectors:**
```css
/* Multiple Selectors */
h1, h2, h3                ‚Üí ‡¶∏‡¶¨ h1, h2, h3
.price, .rating           ‚Üí price ‡¶Ö‡¶•‡¶¨‡¶æ rating class

/* Complex Combinations */
.product-section .price:not(.discount)    ‚Üí product-section ‡¶è‡¶∞ ‡¶≠‡¶ø‡¶§‡¶∞‡ßá‡¶∞ price class ‡¶Ø‡¶æ‡¶∞ discount class ‡¶®‡ßá‡¶á
.container > .product-info span[data-currency] ‚Üí container ‡¶è‡¶∞ direct child product-info ‡¶è‡¶∞ span ‡¶Ø‡¶æ‡¶∞ data-currency ‡¶Ü‡¶õ‡ßá

/* Case-insensitive Attribute */
[data-feature="CAMERA" i] ‚Üí case-insensitive matching
```

### üé≠ Playwright ‡¶è Selector ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞:

#### Text-based Selectors (Playwright Special):
```python
# Text content ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá select ‡¶ï‡¶∞‡¶æ
page.locator('text="Add to Cart"')           # Exact text match
page.locator('text=/Add.*Cart/i')            # Regex match (case-insensitive)
page.locator('"Add to Cart"')                # Shorthand for exact text

# Partial text match
page.locator('text="Add"')                   # "Add" text ‡¶Ü‡¶õ‡ßá ‡¶è‡¶Æ‡¶® element

# Role-based selectors (Accessibility)
page.locator('role=button[name="Add to Cart"]')  # Button role with name
page.locator('role=textbox[name="Search"]')      # Textbox role
page.locator('role=link[name="Home"]')           # Link role
```

#### XPath Selectors:
```python
# XPath ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá complex selection
page.locator('xpath=//div[@class="product-info"]//span[@data-currency="BDT"]')
page.locator('xpath=//button[contains(text(), "Add")]')
page.locator('xpath=//li[position()=2]')     # ‡ß®‡¶Ø‡¶º li element
page.locator('xpath=//div[following-sibling::span[@class="price"]]')  # price class ‡¶è‡¶∞ ‡¶Ü‡¶ó‡ßá‡¶∞ div
```

### üí° Selector Testing ‡¶ì Debugging:

#### DevTools Console ‡¶è Test ‡¶ï‡¶∞‡¶æ:
```javascript
// CSS Selector test
document.querySelector('.product-info .price')
document.querySelectorAll('.feature-item')

// XPath test
$x('//div[@class="product-info"]//span[@data-currency="BDT"]')

// Playwright-style text selector simulation
document.evaluate('//button[contains(text(), "Add to Cart")]', document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue
```

#### Playwright ‡¶è Selector Debugging:
```python
# Selector highlight ‡¶ï‡¶∞‡¶æ
page.locator('.price').highlight()

# Element count check ‡¶ï‡¶∞‡¶æ
count = page.locator('.feature-item').count()
print(f"Found {count} feature items")

# Wait for selector
page.wait_for_selector('.product-info', timeout=5000)

# Check if selector exists
if page.locator('.add-to-cart').is_visible():
    print("Add to cart button is visible")
```

---

## üé≠ Playwright ‡¶¨‡ßá‡¶∏‡¶ø‡¶ï - Browser ‡¶ì Page {#playwright-basics}

### üöÄ Browser Launch Options:

#### Basic Browser Launch:
```python
from playwright.sync_api import sync_playwright

# ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶∏‡¶ø‡¶Æ‡ßç‡¶™‡¶≤ way
with sync_playwright() as p:
    browser = p.chromium.launch()  # Headless mode (default)
    page = browser.new_page()
    page.goto("https://example.com")
    print(page.title())
    browser.close()
```

#### Advanced Browser Configuration:
```python
def advanced_browser_setup():
    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=False,                    # Browser ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡¶¨‡ßá
            slow_mo=1000,                      # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø action ‡¶è 1 ‡¶∏‡ßá‡¶ï‡ßá‡¶®‡ßç‡¶° delay
            devtools=True,                     # DevTools auto open
            args=[
                "--start-maximized",           # Full screen
                "--disable-blink-features=AutomationControlled",  # Bot detection ‡¶è‡¶°‡¶º‡¶æ‡¶®‡ßã
                "--disable-web-security",      # CORS disable
                "--disable-features=VizDisplayCompositor",
                "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            ]
        )

        # Browser context with custom settings
        context = browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            locale='bn-BD',                    # Bengali locale
            timezone_id='Asia/Dhaka',          # Dhaka timezone
            permissions=['geolocation'],       # Location permission
            geolocation={'latitude': 23.8103, 'longitude': 90.4125}  # Dhaka coordinates
        )

        page = context.new_page()
        return browser, context, page

browser, context, page = advanced_browser_setup()
```

### üåê Page Navigation ‡¶ì Loading:

#### Basic Navigation:
```python
# URL ‡¶è ‡¶Ø‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ
page.goto("https://example.com")

# Wait strategies
page.wait_for_load_state("load")          # HTML load complete
page.wait_for_load_state("domcontentloaded")  # DOM ready
page.wait_for_load_state("networkidle")   # Network idle (recommended)

# Page info
print(f"Title: {page.title()}")
print(f"URL: {page.url}")
print(f"Content: {page.content()[:100]}...")  # First 100 chars
```

#### Advanced Navigation:
```python
def smart_navigation(page, url, max_retries=3):
    """Smart navigation with retry logic"""

    for attempt in range(max_retries):
        try:
            # Navigate with timeout
            response = page.goto(url, timeout=30000, wait_until="networkidle")

            # Check response status
            if response.status >= 400:
                print(f"‚ùå HTTP Error: {response.status}")
                continue

            # Wait for critical elements
            page.wait_for_selector('body', timeout=10000)

            # Check if page loaded properly
            if page.locator('body').is_visible():
                print(f"‚úÖ Successfully loaded: {url}")
                return True

        except Exception as e:
            print(f"‚ö†Ô∏è Attempt {attempt + 1} failed: {e}")
            if attempt < max_retries - 1:
                page.wait_for_timeout(2000)  # Wait before retry
                continue

    print(f"‚ùå Failed to load {url} after {max_retries} attempts")
    return False

# Usage
success = smart_navigation(page, "https://example.com")
```

### üîç Element Location ‡¶ì Selection:

#### Modern Locator API:
```python
# Modern way (recommended)
title = page.locator("h1")                    # CSS selector
price = page.locator(".price")                # Class selector
button = page.locator("#submit-btn")          # ID selector

# Text-based locators
add_button = page.locator('text="Add to Cart"')
link = page.locator('text=/Home|‡¶π‡ßã‡¶Æ/i')       # Regex with Bengali

# Role-based locators (Accessibility)
search_box = page.locator('role=textbox[name="Search"]')
nav_links = page.locator('role=link')

# Attribute-based locators
product = page.locator('[data-product-id="123"]')
images = page.locator('img[alt*="product"]')
```

#### Legacy Selectors (still useful):
```python
# Old way (still works)
title_element = page.query_selector("h1")
all_links = page.query_selector_all("a")

# XPath
xpath_element = page.locator('xpath=//div[@class="product"]//span[@class="price"]')
```

### üìä Element Information Extraction:

#### Text Content:
```python
# Text content ‡¶®‡ßá‡¶ì‡¶Ø‡¶º‡¶æ
title_text = page.locator("h1").text_content()
price_text = page.locator(".price").text_content()

# Inner text (visible text only)
visible_text = page.locator(".description").inner_text()

# Inner HTML
html_content = page.locator(".product-info").inner_html()

# All text contents (multiple elements)
all_prices = page.locator(".price").all_text_contents()
print(f"All prices: {all_prices}")
```

#### Attributes:
```python
# Single attribute
link_href = page.locator("a").get_attribute("href")
image_src = page.locator("img").get_attribute("src")
data_id = page.locator(".product").get_attribute("data-id")

# Multiple attributes
element = page.locator(".product-card")
attributes = {
    'id': element.get_attribute('id'),
    'class': element.get_attribute('class'),
    'data-price': element.get_attribute('data-price')
}
print(attributes)
```

#### Element Properties:
```python
# Element visibility
is_visible = page.locator(".popup").is_visible()
is_hidden = page.locator(".loading").is_hidden()

# Element state
is_enabled = page.locator("button").is_enabled()
is_disabled = page.locator("input").is_disabled()
is_checked = page.locator("checkbox").is_checked()

# Element count
product_count = page.locator(".product-card").count()
print(f"Found {product_count} products")
```

### üéØ Multiple Elements Handling:

#### Working with Lists:
```python
# Get all matching elements
products = page.locator(".product-card").all()

# Loop through elements
for i, product in enumerate(products):
    name = product.locator(".product-name").text_content()
    price = product.locator(".product-price").text_content()
    print(f"{i+1}. {name} - {price}")

# Nth element
first_product = page.locator(".product-card").nth(0)  # First
last_product = page.locator(".product-card").nth(-1)  # Last
second_product = page.locator(".product-card").nth(1) # Second

# Filter elements
expensive_products = page.locator(".product-card").filter(has_text="‡ß´‡ß¶,‡ß¶‡ß¶‡ß¶")
available_products = page.locator(".product-card").filter(has=page.locator(".in-stock"))
```

#### Advanced Element Queries:
```python
def extract_product_data(page):
    """Extract all product data from a page"""

    products = []
    product_cards = page.locator(".product-card").all()

    for card in product_cards:
        try:
            product_data = {
                'name': card.locator(".product-name").text_content().strip(),
                'price': card.locator(".product-price").text_content().strip(),
                'rating': card.locator(".rating").text_content().strip() if card.locator(".rating").is_visible() else "No rating",
                'image_url': card.locator("img").get_attribute("src"),
                'product_url': card.locator("a").get_attribute("href"),
                'availability': "In Stock" if card.locator(".in-stock").is_visible() else "Out of Stock"
            }
            products.append(product_data)

        except Exception as e:
            print(f"Error extracting product data: {e}")
            continue

    return products

# Usage
products = extract_product_data(page)
print(f"Extracted {len(products)} products")
```

---

## üñ±Ô∏è Element Interaction - Click, Type, Fill {#element-interaction}

### üéØ Click Operations:

#### Basic Clicking:
```python
# Simple click
page.locator("button").click()
page.locator("#submit-btn").click()
page.locator('text="Add to Cart"').click()

# Click with options
page.locator("button").click(
    button="left",           # left, right, middle
    click_count=1,           # Single click (default)
    delay=100,               # Delay between mousedown and mouseup
    force=True,              # Force click even if element is not visible
    no_wait_after=False,     # Wait for navigation after click
    timeout=30000            # Timeout in milliseconds
)

# Double click
page.locator(".file-item").dblclick()

# Right click (context menu)
page.locator(".item").click(button="right")
```

#### Advanced Click Scenarios:
```python
def smart_click(page, selector, max_attempts=3):
    """Smart clicking with retry logic"""

    for attempt in range(max_attempts):
        try:
            element = page.locator(selector)

            # Wait for element to be visible and enabled
            element.wait_for(state="visible", timeout=10000)

            # Scroll into view if needed
            element.scroll_into_view_if_needed()

            # Check if clickable
            if element.is_enabled():
                element.click()
                print(f"‚úÖ Successfully clicked: {selector}")
                return True
            else:
                print(f"‚ö†Ô∏è Element not enabled: {selector}")

        except Exception as e:
            print(f"‚ùå Click attempt {attempt + 1} failed: {e}")
            if attempt < max_attempts - 1:
                page.wait_for_timeout(1000)
                continue

    return False

# Usage
success = smart_click(page, ".add-to-cart-btn")
```

### ‚å®Ô∏è Text Input Operations:

#### Basic Text Input:
```python
# Fill input field (clears first, then types)
page.locator('input[name="username"]').fill("my_username")
page.locator('#password').fill("my_password")
page.locator('textarea').fill("This is a long text message")

# Type text (simulates human typing)
page.locator('input[type="search"]').type("laptop computer", delay=100)

# Clear input
page.locator('input').clear()

# Press specific keys
page.locator('input').press("Enter")
page.locator('input').press("Tab")
page.locator('input').press("Escape")
page.locator('input').press("Control+A")  # Select all
```

#### Advanced Text Input:
```python
def fill_form_data(page, form_data):
    """Fill multiple form fields"""

    for field_name, value in form_data.items():
        try:
            # Try different selector patterns
            selectors = [
                f'input[name="{field_name}"]',
                f'#{field_name}',
                f'.{field_name}',
                f'input[placeholder*="{field_name}"]'
            ]

            element_found = False
            for selector in selectors:
                element = page.locator(selector)
                if element.is_visible():
                    element.fill(str(value))
                    print(f"‚úÖ Filled {field_name}: {value}")
                    element_found = True
                    break

            if not element_found:
                print(f"‚ùå Field not found: {field_name}")

        except Exception as e:
            print(f"‚ùå Error filling {field_name}: {e}")

# Usage
form_data = {
    "username": "john_doe",
    "email": "john@example.com",
    "phone": "01712345678",
    "address": "Dhaka, Bangladesh"
}

fill_form_data(page, form_data)
```

### üìã Form Interactions:

#### Dropdown/Select Operations:
```python
# Select by value
page.locator('select[name="country"]').select_option("BD")

# Select by label
page.locator('select').select_option(label="Bangladesh")

# Select by index
page.locator('select').select_option(index=2)

# Multiple selections
page.locator('select[multiple]').select_option(["option1", "option2"])

# Get selected value
selected = page.locator('select').input_value()
print(f"Selected: {selected}")
```

#### Checkbox and Radio Operations:
```python
# Check checkbox
page.locator('input[type="checkbox"]').check()

# Uncheck checkbox
page.locator('input[type="checkbox"]').uncheck()

# Set checkbox state
page.locator('input[type="checkbox"]').set_checked(True)

# Radio button selection
page.locator('input[value="male"]').check()

# Check if checked
is_checked = page.locator('input[type="checkbox"]').is_checked()
print(f"Checkbox is checked: {is_checked}")
```

#### File Upload:
```python
# Single file upload
page.locator('input[type="file"]').set_input_files("path/to/file.pdf")

# Multiple files upload
page.locator('input[type="file"]').set_input_files([
    "file1.jpg",
    "file2.png",
    "document.pdf"
])

# Remove files
page.locator('input[type="file"]').set_input_files([])
```

### üñ±Ô∏è Mouse Operations:

#### Hover and Mouse Events:
```python
# Hover over element
page.locator(".menu-item").hover()

# Hover with custom position
page.locator(".element").hover(position={"x": 50, "y": 50})

# Mouse down and up
page.locator(".draggable").mouse_down()
page.locator(".drop-zone").mouse_up()

# Drag and drop
page.locator(".source").drag_to(page.locator(".target"))
```

### ‚è±Ô∏è Wait Strategies:

#### Element-based Waits:
```python
# Wait for element to be visible
page.locator(".loading").wait_for(state="visible")

# Wait for element to be hidden
page.locator(".popup").wait_for(state="hidden")

# Wait for element to be attached to DOM
page.locator(".dynamic-content").wait_for(state="attached")

# Wait for element to be detached from DOM
page.locator(".temporary-element").wait_for(state="detached")

# Wait with timeout
try:
    page.locator(".slow-element").wait_for(state="visible", timeout=5000)
except TimeoutError:
    print("Element did not appear within 5 seconds")
```

#### Custom Wait Conditions:
```python
def wait_for_element_count(page, selector, expected_count, timeout=10000):
    """Wait until element count matches expected"""

    def check_count():
        actual_count = page.locator(selector).count()
        return actual_count == expected_count

    page.wait_for_function(
        f"() => document.querySelectorAll('{selector}').length === {expected_count}",
        timeout=timeout
    )

# Wait for specific text to appear
def wait_for_text(page, selector, expected_text, timeout=10000):
    """Wait for element to contain specific text"""

    page.wait_for_function(
        f"""() => {{
            const element = document.querySelector('{selector}');
            return element && element.textContent.includes('{expected_text}');
        }}""",
        timeout=timeout
    )

# Usage
wait_for_element_count(page, ".product-card", 20)
wait_for_text(page, ".status", "Loading complete")
```

### üéØ Complex Interaction Scenarios:

#### Login Automation:
```python
def automated_login(page, username, password):
    """Automated login with error handling"""

    try:
        # Navigate to login page
        page.goto("https://example.com/login")

        # Wait for login form
        page.wait_for_selector('form', timeout=10000)

        # Fill credentials
        page.locator('input[name="username"]').fill(username)
        page.locator('input[name="password"]').fill(password)

        # Submit form
        page.locator('button[type="submit"]').click()

        # Wait for navigation or error message
        try:
            # Success: wait for dashboard
            page.wait_for_url("**/dashboard", timeout=5000)
            print("‚úÖ Login successful!")
            return True

        except:
            # Check for error message
            error_element = page.locator('.error-message')
            if error_element.is_visible():
                error_text = error_element.text_content()
                print(f"‚ùå Login failed: {error_text}")
            else:
                print("‚ùå Login failed: Unknown error")
            return False

    except Exception as e:
        print(f"‚ùå Login error: {e}")
        return False

# Usage
success = automated_login(page, "my_username", "my_password")
```

#### Search and Filter:
```python
def search_and_filter(page, search_term, filters=None):
    """Search with filters and return results"""

    # Perform search
    search_box = page.locator('input[type="search"]')
    search_box.fill(search_term)
    search_box.press("Enter")

    # Wait for results
    page.wait_for_selector('.search-results', timeout=10000)

    # Apply filters if provided
    if filters:
        for filter_type, filter_value in filters.items():
            filter_selector = f'select[name="{filter_type}"]'
            if page.locator(filter_selector).is_visible():
                page.locator(filter_selector).select_option(filter_value)
                page.wait_for_load_state('networkidle')

    # Extract results
    results = []
    result_elements = page.locator('.search-result').all()

    for element in result_elements:
        result_data = {
            'title': element.locator('.title').text_content(),
            'description': element.locator('.description').text_content(),
            'url': element.locator('a').get_attribute('href')
        }
        results.append(result_data)

    return results

# Usage
filters = {"category": "electronics", "price_range": "10000-50000"}
results = search_and_filter(page, "smartphone", filters)
print(f"Found {len(results)} results")
```
```

### üìö Requests + BeautifulSoup (‡¶¨‡ßá‡¶∏‡¶ø‡¶ï):

#### ‡¶∏‡¶ø‡¶Æ‡ßç‡¶™‡¶≤ ‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶™‡¶ø‡¶Ç:
```python
import requests
from bs4 import BeautifulSoup
import time

def scrape_basic_site():
    url = "https://quotes.toscrape.com/"
    
    # Headers ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ (bot detection ‡¶è‡¶°‡¶º‡¶æ‡¶®‡ßã‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    response = requests.get(url, headers=headers)
    
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ‡¶∏‡¶¨ quotes ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π
        quotes = soup.find_all('div', class_='quote')
        
        for quote in quotes:
            text = quote.find('span', class_='text').text
            author = quote.find('small', class_='author').text
            tags = [tag.text for tag in quote.find_all('a', class_='tag')]
            
            print(f"Quote: {text}")
            print(f"Author: {author}")
            print(f"Tags: {', '.join(tags)}")
            print("-" * 50)
    
    else:
        print(f"Error: {response.status_code}")

# ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ‡¶ö‡¶æ‡¶≤‡¶æ‡¶®‡ßã
scrape_basic_site()
```

#### Multiple Pages ‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶™‡¶ø‡¶Ç:
```python
def scrape_multiple_pages():
    base_url = "https://quotes.toscrape.com/page/{}"
    all_quotes = []
    
    for page in range(1, 6):  # ‡ßß-‡ß´ ‡¶™‡ßá‡¶ú
        url = base_url.format(page)
        print(f"Scraping page {page}...")
        
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        quotes = soup.find_all('div', class_='quote')
        
        if not quotes:  # ‡¶Ü‡¶∞ ‡¶ï‡ßã‡¶® quote ‡¶®‡ßá‡¶á
            break
            
        for quote in quotes:
            quote_data = {
                'text': quote.find('span', class_='text').text,
                'author': quote.find('small', class_='author').text,
                'tags': [tag.text for tag in quote.find_all('a', class_='tag')]
            }
            all_quotes.append(quote_data)
        
        # ‡¶∏‡¶æ‡¶á‡¶ü‡ßá‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶ö‡¶æ‡¶™ ‡¶ï‡¶Æ‡¶æ‡¶®‡ßã‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø delay
        time.sleep(1)
    
    return all_quotes

quotes = scrape_multiple_pages()
print(f"Total quotes collected: {len(quotes)}")
```

### üîÑ Session ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ (Login ‡¶∏‡¶π):
```python
import requests
from bs4 import BeautifulSoup

def login_and_scrape():
    session = requests.Session()

    # Login page ‡¶•‡ßá‡¶ï‡ßá CSRF token ‡¶®‡ßá‡¶ì‡¶Ø‡¶º‡¶æ
    login_url = "https://example.com/login"
    login_page = session.get(login_url)
    soup = BeautifulSoup(login_page.content, 'html.parser')
    csrf_token = soup.find('input', {'name': 'csrf_token'})['value']

    # Login data
    login_data = {
        'username': 'your_username',
        'password': 'your_password',
        'csrf_token': csrf_token
    }

    # Login ‡¶ï‡¶∞‡¶æ
    session.post(login_url, data=login_data)

    # Protected page access
    protected_url = "https://example.com/protected"
    response = session.get(protected_url)

    if "dashboard" in response.text.lower():
        print("Successfully logged in!")
        # ‡¶è‡¶ñ‡¶® scraping ‡¶ï‡¶∞‡ßÅ‡¶®
    else:
        print("Login failed!")

login_and_scrape()
```

### üìä Error Handling ‡¶ì Retry Logic:
```python
import requests
from bs4 import BeautifulSoup
import time
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

def create_session_with_retries():
    session = requests.Session()

    # Retry strategy
    retry_strategy = Retry(
        total=3,                    # ‡¶Æ‡ßã‡¶ü ‡ß© ‡¶¨‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ
        backoff_factor=1,           # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶¨‡¶æ‡¶∞ ‡ßß ‡¶∏‡ßá‡¶ï‡ßá‡¶®‡ßç‡¶° ‡¶¨‡ßá‡¶∂‡¶ø ‡¶Ö‡¶™‡ßá‡¶ï‡ßç‡¶∑‡¶æ
        status_forcelist=[429, 500, 502, 503, 504],  # ‡¶è‡¶á status codes ‡¶è retry
    )

    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)

    return session

def robust_scraping():
    session = create_session_with_retries()

    urls = [
        "https://example1.com",
        "https://example2.com",
        "https://example3.com"
    ]

    for url in urls:
        try:
            response = session.get(url, timeout=10)
            response.raise_for_status()  # HTTP error ‡¶•‡¶æ‡¶ï‡¶≤‡ßá exception raise ‡¶ï‡¶∞‡¶¨‡ßá

            soup = BeautifulSoup(response.content, 'html.parser')
            # Scraping logic here

        except requests.exceptions.RequestException as e:
            print(f"Error scraping {url}: {e}")
            continue

        time.sleep(2)  # Rate limiting

robust_scraping()
```

---

## üé≠ Playwright ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶°‡¶≠‡¶æ‡¶®‡ßç‡¶∏ ‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶™‡¶ø‡¶Ç {#playwright-advanced}

### üöÄ Playwright ‡¶ï‡ßá‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶¨‡ßá‡¶®?
- **JavaScript ‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶æ‡¶∞‡¶ø‡¶Ç:** Dynamic content load ‡¶π‡¶Ø‡¶º
- **User Interaction:** Click, scroll, form fill ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®
- **Multiple Browsers:** Chrome, Firefox, Safari support
- **Mobile Emulation:** Mobile device simulate ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®

### üéØ ‡¶¨‡ßá‡¶∏‡¶ø‡¶ï Playwright Setup:
```python
from playwright.sync_api import sync_playwright
import time

def basic_playwright_scraping():
    with sync_playwright() as p:
        # Browser launch (headless=False ‡¶Æ‡¶æ‡¶®‡ßá browser ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡¶¨‡ßá)
        browser = p.chromium.launch(
            headless=False,
            args=["--auto-open-devtools-for-tabs"]
        )

        # New page create
        page = browser.new_page()

        # URL ‡¶è ‡¶Ø‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ
        page.goto("https://example.com")

        # Page load ‡¶π‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ö‡¶™‡ßá‡¶ï‡ßç‡¶∑‡¶æ
        page.wait_for_load_state("networkidle")

        # Title print
        print(f"Page title: {page.title()}")

        # Element ‡¶•‡ßá‡¶ï‡ßá text ‡¶®‡ßá‡¶ì‡¶Ø‡¶º‡¶æ
        heading = page.locator("h1").text_content()
        print(f"Main heading: {heading}")

        # Screenshot ‡¶®‡ßá‡¶ì‡¶Ø‡¶º‡¶æ
        page.screenshot(path="screenshot.png")

        browser.close()

basic_playwright_scraping()
```

### üñ±Ô∏è User Interactions:
```python
def interactive_scraping():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()

        page.goto("https://example.com/search")

        # Search box ‡¶è text type ‡¶ï‡¶∞‡¶æ
        page.fill('input[name="search"]', 'Python programming')

        # Search button ‡¶è click
        page.click('button[type="submit"]')

        # Results load ‡¶π‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ö‡¶™‡ßá‡¶ï‡ßç‡¶∑‡¶æ
        page.wait_for_selector('.search-results')

        # Results scrape ‡¶ï‡¶∞‡¶æ
        results = page.locator('.search-result').all()

        for i, result in enumerate(results):
            title = result.locator('.title').text_content()
            link = result.locator('a').get_attribute('href')
            print(f"{i+1}. {title} - {link}")

        browser.close()

interactive_scraping()
```

### üì± Mobile Device Emulation:
```python
def mobile_scraping():
    with sync_playwright() as p:
        # Mobile device emulate ‡¶ï‡¶∞‡¶æ
        iphone = p.devices['iPhone 12']
        browser = p.chromium.launch()
        context = browser.new_context(**iphone)
        page = context.new_page()

        page.goto("https://example.com")

        # Mobile specific elements scrape ‡¶ï‡¶∞‡¶æ
        mobile_menu = page.locator('.mobile-menu')
        if mobile_menu.is_visible():
            mobile_menu.click()

        browser.close()

mobile_scraping()
```

### üîÑ Infinite Scroll Handling:
```python
def scrape_infinite_scroll():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()

        page.goto("https://example.com/infinite-scroll")

        # Initial items count
        previous_count = 0

        while True:
            # Current items count
            current_count = page.locator('.item').count()

            if current_count == previous_count:
                print("No more items to load")
                break

            print(f"Loaded {current_count} items")
            previous_count = current_count

            # Scroll to bottom
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")

            # Wait for new content
            page.wait_for_timeout(2000)

        # Scrape all items
        items = page.locator('.item').all()
        for item in items:
            title = item.locator('.title').text_content()
            print(title)

        browser.close()

scrape_infinite_scroll()
```

### üç™ Cookies ‡¶ì Local Storage:
```python
def handle_cookies_storage():
    with sync_playwright() as p:
        browser = p.chromium.launch()
        context = browser.new_context()
        page = context.new_page()

        # Cookie set ‡¶ï‡¶∞‡¶æ
        context.add_cookies([{
            'name': 'session_id',
            'value': 'abc123',
            'domain': 'example.com',
            'path': '/'
        }])

        page.goto("https://example.com")

        # Local storage ‡¶è data set ‡¶ï‡¶∞‡¶æ
        page.evaluate("""
            localStorage.setItem('user_preference', 'dark_mode');
        """)

        # Cookie read ‡¶ï‡¶∞‡¶æ
        cookies = context.cookies()
        for cookie in cookies:
            print(f"{cookie['name']}: {cookie['value']}")

        # Local storage read ‡¶ï‡¶∞‡¶æ
        user_pref = page.evaluate("localStorage.getItem('user_preference')")
        print(f"User preference: {user_pref}")

        browser.close()

handle_cookies_storage()
```

### üö´ Request Blocking (Ad Block):
```python
def block_ads_and_trackers():
    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()

        # Block ‡¶ï‡¶∞‡¶æ‡¶∞ domain list
        blocked_domains = [
            "doubleclick.net",
            "googlesyndication.com",
            "googletagmanager.com",
            "facebook.com/tr",
            "google-analytics.com"
        ]

        def handle_route(route, request):
            # Check if request URL contains blocked domains
            if any(domain in request.url for domain in blocked_domains):
                print(f"Blocked: {request.url}")
                route.abort()
            else:
                route.continue_()

        # All requests intercept ‡¶ï‡¶∞‡¶æ
        page.route("**/*", handle_route)

        page.goto("https://example.com")

        # Page content scrape ‡¶ï‡¶∞‡¶æ (ads ‡¶õ‡¶æ‡¶°‡¶º‡¶æ)
        content = page.locator('.main-content').text_content()
        print(content)

        browser.close()

block_ads_and_trackers()
```

---

## üíæ ‡¶°‡ßá‡¶ü‡¶æ ‡¶∏‡¶Ç‡¶∞‡¶ï‡ßç‡¶∑‡¶£ ‡¶ì ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç {#data-processing}

### üìä CSV ‡¶´‡¶æ‡¶á‡¶≤‡ßá ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡¶æ:
```python
import csv
import pandas as pd

def save_to_csv(data, filename):
    """
    data: list of dictionaries
    filename: CSV file name
    """
    if not data:
        print("No data to save!")
        return

    # CSV writer ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá
    with open(filename, 'w', newline='', encoding='utf-8') as file:
        fieldnames = data[0].keys()
        writer = csv.DictWriter(file, fieldnames=fieldnames)

        writer.writeheader()
        writer.writerows(data)

    print(f"Data saved to {filename}")

# Example usage
scraped_data = [
    {'name': 'Product 1', 'price': '‡ß´‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ', 'rating': '4.5'},
    {'name': 'Product 2', 'price': '‡ß≠‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ', 'rating': '4.2'},
]

save_to_csv(scraped_data, 'products.csv')
```

### üêº Pandas ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶°‡ßá‡¶ü‡¶æ ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç:
```python
import pandas as pd
import numpy as np

def process_scraped_data():
    # CSV ‡¶•‡ßá‡¶ï‡ßá data load ‡¶ï‡¶∞‡¶æ
    df = pd.read_csv('products.csv')

    # Data cleaning
    df['price_numeric'] = df['price'].str.extract('(\d+)').astype(int)
    df['rating_numeric'] = df['rating'].astype(float)

    # Data analysis
    print("=== Data Summary ===")
    print(f"Total products: {len(df)}")
    print(f"Average price: {df['price_numeric'].mean():.2f} ‡¶ü‡¶æ‡¶ï‡¶æ")
    print(f"Average rating: {df['rating_numeric'].mean():.2f}")

    # Filter high-rated products
    high_rated = df[df['rating_numeric'] >= 4.0]
    print(f"High-rated products: {len(high_rated)}")

    # Save processed data
    df.to_csv('processed_products.csv', index=False)

    return df

processed_df = process_scraped_data()
```

### üóÑÔ∏è Database ‡¶è ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡¶æ (SQLite):
```python
import sqlite3
import pandas as pd

def save_to_database(data, db_name='scraped_data.db'):
    # Database connection
    conn = sqlite3.connect(db_name)
    cursor = conn.cursor()

    # Table create ‡¶ï‡¶∞‡¶æ
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS products (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT NOT NULL,
            price TEXT,
            rating REAL,
            scraped_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')

    # Data insert ‡¶ï‡¶∞‡¶æ
    for item in data:
        cursor.execute('''
            INSERT INTO products (name, price, rating)
            VALUES (?, ?, ?)
        ''', (item['name'], item['price'], float(item['rating'])))

    conn.commit()
    conn.close()
    print(f"Data saved to database: {db_name}")

# Database ‡¶•‡ßá‡¶ï‡ßá data read ‡¶ï‡¶∞‡¶æ
def read_from_database(db_name='scraped_data.db'):
    conn = sqlite3.connect(db_name)
    df = pd.read_sql_query("SELECT * FROM products", conn)
    conn.close()
    return df

# Usage
scraped_data = [
    {'name': 'Product 1', 'price': '‡ß´‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ', 'rating': '4.5'},
    {'name': 'Product 2', 'price': '‡ß≠‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ', 'rating': '4.2'},
]

save_to_database(scraped_data)
db_data = read_from_database()
print(db_data)
```

### üìà ‡¶°‡ßá‡¶ü‡¶æ ‡¶≠‡¶ø‡¶ú‡ßÅ‡¶Ø‡¶º‡¶æ‡¶≤‡¶æ‡¶á‡¶ú‡ßá‡¶∂‡¶®:
```python
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_data(df):
    # Set Bengali font (if available)
    plt.rcParams['font.family'] = ['DejaVu Sans']

    # Price distribution
    plt.figure(figsize=(12, 8))

    plt.subplot(2, 2, 1)
    plt.hist(df['price_numeric'], bins=20, alpha=0.7)
    plt.title('Price Distribution')
    plt.xlabel('Price (Taka)')
    plt.ylabel('Frequency')

    # Rating distribution
    plt.subplot(2, 2, 2)
    plt.hist(df['rating_numeric'], bins=10, alpha=0.7, color='orange')
    plt.title('Rating Distribution')
    plt.xlabel('Rating')
    plt.ylabel('Frequency')

    # Price vs Rating scatter plot
    plt.subplot(2, 2, 3)
    plt.scatter(df['price_numeric'], df['rating_numeric'], alpha=0.6)
    plt.title('Price vs Rating')
    plt.xlabel('Price (Taka)')
    plt.ylabel('Rating')

    # Top 10 products by rating
    plt.subplot(2, 2, 4)
    top_products = df.nlargest(10, 'rating_numeric')
    plt.barh(range(len(top_products)), top_products['rating_numeric'])
    plt.title('Top 10 Products by Rating')
    plt.xlabel('Rating')

    plt.tight_layout()
    plt.savefig('data_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()

# Usage
df = pd.read_csv('processed_products.csv')
visualize_data(df)
```

---

## üîß ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶∏‡¶Æ‡¶æ‡¶ß‡¶æ‡¶® ‡¶ì ‡¶ü‡¶ø‡¶™‡¶∏ {#troubleshooting}

### üö´ ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶ì ‡¶∏‡¶Æ‡¶æ‡¶ß‡¶æ‡¶®:

#### ‡ßß. **Bot Detection ‡¶è‡¶°‡¶º‡¶æ‡¶®‡ßã:**
```python
import random
import time
from fake_useragent import UserAgent

def avoid_bot_detection():
    ua = UserAgent()

    headers = {
        'User-Agent': ua.random,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }

    # Random delay between requests
    delay = random.uniform(1, 3)
    time.sleep(delay)

    return headers

# Playwright ‡¶è User-Agent change ‡¶ï‡¶∞‡¶æ
def playwright_avoid_detection():
    with sync_playwright() as p:
        browser = p.chromium.launch()
        context = browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        )
        page = context.new_page()

        # Additional headers
        page.set_extra_http_headers({
            'Accept-Language': 'en-US,en;q=0.9',
        })

        page.goto("https://example.com")
        browser.close()
```

#### ‡ß®. **CAPTCHA ‡¶∏‡¶Æ‡¶æ‡¶ß‡¶æ‡¶®:**
```python
# Manual CAPTCHA solving
def handle_captcha_manually():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)  # headless=False ‡¶∞‡¶æ‡¶ñ‡ßÅ‡¶®
        page = browser.new_page()

        page.goto("https://example.com")

        # CAPTCHA ‡¶Ü‡¶õ‡ßá ‡¶ï‡¶ø‡¶®‡¶æ check ‡¶ï‡¶∞‡¶æ
        if page.locator('.captcha').is_visible():
            print("CAPTCHA detected! Please solve it manually...")

            # User ‡¶è‡¶∞ CAPTCHA solve ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ö‡¶™‡ßá‡¶ï‡ßç‡¶∑‡¶æ
            page.wait_for_selector('.captcha', state='hidden', timeout=60000)
            print("CAPTCHA solved! Continuing...")

        # Continue scraping
        browser.close()
```

#### ‡ß©. **Rate Limiting Handle ‡¶ï‡¶∞‡¶æ:**
```python
import time
from datetime import datetime, timedelta

class RateLimiter:
    def __init__(self, max_requests=10, time_window=60):
        self.max_requests = max_requests
        self.time_window = time_window
        self.requests = []

    def wait_if_needed(self):
        now = datetime.now()

        # Remove old requests
        self.requests = [req_time for req_time in self.requests
                        if now - req_time < timedelta(seconds=self.time_window)]

        # Check if we need to wait
        if len(self.requests) >= self.max_requests:
            sleep_time = self.time_window - (now - self.requests[0]).seconds
            print(f"Rate limit reached. Waiting {sleep_time} seconds...")
            time.sleep(sleep_time)

        self.requests.append(now)

# Usage
rate_limiter = RateLimiter(max_requests=5, time_window=60)

def scrape_with_rate_limit(urls):
    for url in urls:
        rate_limiter.wait_if_needed()

        # Your scraping code here
        response = requests.get(url)
        print(f"Scraped: {url}")
```

#### ‡ß™. **Dynamic Content Loading:**
```python
def wait_for_dynamic_content():
    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()

        page.goto("https://example.com")

        # Method 1: Wait for specific element
        page.wait_for_selector('.dynamic-content', timeout=10000)

        # Method 2: Wait for network to be idle
        page.wait_for_load_state('networkidle')

        # Method 3: Wait for JavaScript to complete
        page.wait_for_function("document.readyState === 'complete'")

        # Method 4: Custom wait condition
        page.wait_for_function("""
            () => document.querySelectorAll('.item').length > 10
        """)

        browser.close()
```

### üí° **Performance Optimization ‡¶ü‡¶ø‡¶™‡¶∏:**

#### ‡ßß. **Concurrent Scraping:**
```python
import asyncio
from playwright.async_api import async_playwright

async def scrape_page(url, semaphore):
    async with semaphore:  # Limit concurrent requests
        async with async_playwright() as p:
            browser = await p.chromium.launch()
            page = await browser.new_page()

            await page.goto(url)
            title = await page.title()

            await browser.close()
            return {'url': url, 'title': title}

async def concurrent_scraping():
    urls = [
        "https://example1.com",
        "https://example2.com",
        "https://example3.com",
    ]

    # Maximum 3 concurrent requests
    semaphore = asyncio.Semaphore(3)

    tasks = [scrape_page(url, semaphore) for url in urls]
    results = await asyncio.gather(*tasks)

    return results

# Run async function
results = asyncio.run(concurrent_scraping())
print(results)
```

#### ‡ß®. **Memory Management:**
```python
def memory_efficient_scraping():
    with sync_playwright() as p:
        browser = p.chromium.launch()

        for i in range(100):  # Many pages
            page = browser.new_page()

            try:
                page.goto(f"https://example.com/page/{i}")
                # Scraping logic

            finally:
                page.close()  # Important: Close page to free memory

        browser.close()
```

---

## ‚öñÔ∏è ‡¶Ü‡¶á‡¶®‡¶ø ‡¶ì ‡¶®‡ßà‡¶§‡¶ø‡¶ï ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º {#legal-ethics}

### üìã **‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶™‡¶ø‡¶Ç ‡¶è‡¶∞ ‡¶®‡¶ø‡¶Ø‡¶º‡¶Æ‡¶ï‡¶æ‡¶®‡ßÅ‡¶®:**

#### ‚úÖ **‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®:**
- **Public Data:** ‡¶∏‡¶¨‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶â‡¶®‡ßç‡¶Æ‡ßÅ‡¶ï‡ßç‡¶§ ‡¶§‡¶•‡ßç‡¶Ø ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π
- **Personal Use:** ‡¶®‡¶ø‡¶ú‡ßá‡¶∞ ‡¶ó‡¶¨‡ßá‡¶∑‡¶£‡¶æ ‡¶¨‡¶æ ‡¶∂‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
- **robots.txt ‡¶Æ‡ßá‡¶®‡ßá ‡¶ö‡¶≤‡¶æ:** ‡¶∏‡¶æ‡¶á‡¶ü‡ßá‡¶∞ ‡¶®‡¶ø‡¶Ø‡¶º‡¶Æ ‡¶Ö‡¶®‡ßÅ‡¶∏‡¶∞‡¶£
- **Rate Limiting:** ‡¶∏‡¶æ‡¶á‡¶ü‡ßá‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶ö‡¶æ‡¶™ ‡¶ï‡¶Æ ‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ

#### ‚ùå **‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶¨‡ßá‡¶® ‡¶®‡¶æ:**
- **Copyright Content:** ‡¶ï‡¶™‡¶ø‡¶∞‡¶æ‡¶á‡¶ü ‡¶∏‡ßÅ‡¶∞‡¶ï‡ßç‡¶∑‡¶ø‡¶§ ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü ‡¶ö‡ßÅ‡¶∞‡¶ø
- **Personal Information:** ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶ó‡¶§ ‡¶§‡¶•‡ßç‡¶Ø ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π
- **Commercial Misuse:** ‡¶Ö‡¶®‡ßÅ‡¶Æ‡¶§‡¶ø ‡¶õ‡¶æ‡¶°‡¶º‡¶æ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡¶æ‡¶Ø‡¶º‡¶ø‡¶ï ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞
- **Server Overload:** ‡¶∏‡¶æ‡¶á‡¶ü ‡¶ï‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶∂ ‡¶ï‡¶∞‡¶æ‡¶®‡ßã

### ü§ñ **robots.txt ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ:**
```python
import requests
from urllib.robotparser import RobotFileParser

def check_robots_txt(url, user_agent='*'):
    """
    Check if scraping is allowed according to robots.txt
    """
    try:
        robots_url = f"{url.rstrip('/')}/robots.txt"

        rp = RobotFileParser()
        rp.set_url(robots_url)
        rp.read()

        # Check if URL can be fetched
        can_fetch = rp.can_fetch(user_agent, url)

        print(f"Can scrape {url}: {can_fetch}")

        # Get crawl delay
        crawl_delay = rp.crawl_delay(user_agent)
        if crawl_delay:
            print(f"Recommended delay: {crawl_delay} seconds")

        return can_fetch, crawl_delay

    except Exception as e:
        print(f"Error checking robots.txt: {e}")
        return True, None  # If can't check, assume allowed

# Usage
can_scrape, delay = check_robots_txt("https://example.com")
if can_scrape:
    print("Scraping is allowed!")
else:
    print("Scraping is not allowed by robots.txt")
```

### üìú **Terms of Service ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ:**
```python
def ethical_scraping_guidelines():
    guidelines = """
    üîç ‡¶®‡ßà‡¶§‡¶ø‡¶ï ‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶™‡¶ø‡¶Ç ‡¶ó‡¶æ‡¶á‡¶°‡¶≤‡¶æ‡¶á‡¶®:

    1. üìñ Terms of Service ‡¶™‡¶°‡¶º‡ßÅ‡¶®
    2. ü§ñ robots.txt ‡¶Æ‡ßá‡¶®‡ßá ‡¶ö‡¶≤‡ßÅ‡¶®
    3. ‚è±Ô∏è Request ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá delay ‡¶∞‡¶æ‡¶ñ‡ßÅ‡¶®
    4. üîí Personal data ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡¶¨‡ßá‡¶® ‡¶®‡¶æ
    5. üí∞ Commercial use ‡¶è‡¶∞ ‡¶Ü‡¶ó‡ßá permission ‡¶®‡¶ø‡¶®
    6. üìä Data ‡¶è‡¶∞ source credit ‡¶¶‡¶ø‡¶®
    7. üîÑ Reasonable frequency ‡¶§‡ßá scrape ‡¶ï‡¶∞‡ßÅ‡¶®
    8. üö´ Site ‡¶è‡¶∞ functionality ‡¶ï‡ßç‡¶∑‡¶§‡¶ø ‡¶ï‡¶∞‡¶¨‡ßá‡¶® ‡¶®‡¶æ
    """
    print(guidelines)

ethical_scraping_guidelines()
```

---

## üèóÔ∏è ‡¶™‡ßç‡¶∞‡¶ú‡ßá‡¶ï‡ßç‡¶ü ‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡¶ö‡¶æ‡¶∞ ‡¶ì ‡¶¨‡ßá‡¶∏‡ßç‡¶ü ‡¶™‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶ï‡¶ü‡¶ø‡¶∏

### üìÅ **Recommended Project Structure:**
```
web_scraping_project/
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ scrapers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_scraper.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ecommerce_scraper.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ news_scraper.py
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ helpers.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_processor.py
‚îÇ   ‚îî‚îÄ‚îÄ config/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ settings.py
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îî‚îÄ‚îÄ exports/
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ test_scrapers.py
‚îÇ
‚îú‚îÄ‚îÄ logs/
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ main.py
```

### üîß **Base Scraper Class:**
```python
# src/scrapers/base_scraper.py
import logging
import time
import random
from abc import ABC, abstractmethod
from typing import List, Dict, Any

class BaseScraper(ABC):
    def __init__(self, base_url: str, delay_range: tuple = (1, 3)):
        self.base_url = base_url
        self.delay_range = delay_range
        self.session = None
        self.setup_logging()

    def setup_logging(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('logs/scraper.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(self.__class__.__name__)

    def random_delay(self):
        """Random delay between requests"""
        delay = random.uniform(*self.delay_range)
        time.sleep(delay)

    @abstractmethod
    def scrape(self) -> List[Dict[str, Any]]:
        """Main scraping method - must be implemented by subclasses"""
        pass

    def save_data(self, data: List[Dict], filename: str):
        """Save scraped data to file"""
        import json
        with open(f'data/raw/{filename}', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        self.logger.info(f"Data saved to {filename}")

# Example implementation
class EcommerceScraper(BaseScraper):
    def scrape(self) -> List[Dict[str, Any]]:
        products = []

        # Your scraping logic here
        self.logger.info("Starting e-commerce scraping...")

        # Example data
        products.append({
            'name': 'Sample Product',
            'price': '‡ß´‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ',
            'rating': 4.5
        })

        return products
```

### ‚öôÔ∏è **Configuration Management:**
```python
# src/config/settings.py
import os
from dataclasses import dataclass
from typing import List

@dataclass
class ScrapingConfig:
    # Browser settings
    headless: bool = True
    browser_timeout: int = 30000

    # Request settings
    max_retries: int = 3
    delay_range: tuple = (1, 3)
    concurrent_requests: int = 5

    # Data settings
    output_format: str = 'json'  # json, csv, xlsx
    data_dir: str = 'data'

    # Logging
    log_level: str = 'INFO'
    log_file: str = 'logs/scraper.log'

    # User agents
    user_agents: List[str] = None

    def __post_init__(self):
        if self.user_agents is None:
            self.user_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
            ]

# Load config from environment
def load_config():
    return ScrapingConfig(
        headless=os.getenv('HEADLESS', 'true').lower() == 'true',
        max_retries=int(os.getenv('MAX_RETRIES', '3')),
        concurrent_requests=int(os.getenv('CONCURRENT_REQUESTS', '5'))
    )
```

### üß™ **Testing:**
```python
# tests/test_scrapers.py
import unittest
from unittest.mock import Mock, patch
from src.scrapers.ecommerce_scraper import EcommerceScraper

class TestEcommerceScraper(unittest.TestCase):
    def setUp(self):
        self.scraper = EcommerceScraper("https://example.com")

    def test_scraper_initialization(self):
        self.assertEqual(self.scraper.base_url, "https://example.com")
        self.assertIsNotNone(self.scraper.logger)

    @patch('requests.get')
    def test_scrape_method(self, mock_get):
        # Mock response
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.content = "<html><body>Test</body></html>"
        mock_get.return_value = mock_response

        # Test scraping
        result = self.scraper.scrape()
        self.assertIsInstance(result, list)

    def test_random_delay(self):
        import time
        start_time = time.time()
        self.scraper.random_delay()
        end_time = time.time()

        # Check if delay was applied
        self.assertGreater(end_time - start_time, 0.5)

if __name__ == '__main__':
    unittest.main()
```

---

## üåü ‡¶∞‡¶ø‡¶Ø‡¶º‡ßá‡¶≤-‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞‡ßç‡¶≤‡ßç‡¶° ‡¶™‡ßç‡¶∞‡¶ú‡ßá‡¶ï‡ßç‡¶ü ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£

### üõí **E-commerce Price Monitor:**
```python
# main.py - E-commerce price monitoring system
import schedule
import time
from datetime import datetime
from src.scrapers.ecommerce_scraper import EcommerceScraper
from src.utils.data_processor import DataProcessor
from src.utils.helpers import send_notification

class PriceMonitor:
    def __init__(self):
        self.scraper = EcommerceScraper("https://example-shop.com")
        self.processor = DataProcessor()
        self.target_products = [
            {'name': 'iPhone 15', 'target_price': 80000},
            {'name': 'Samsung Galaxy S24', 'target_price': 70000}
        ]

    def monitor_prices(self):
        print(f"üîç Price monitoring started at {datetime.now()}")

        # Scrape current prices
        current_data = self.scraper.scrape()

        # Process and compare with targets
        for product in current_data:
            for target in self.target_products:
                if target['name'].lower() in product['name'].lower():
                    current_price = self.extract_price(product['price'])

                    if current_price <= target['target_price']:
                        message = f"üéâ Price Alert! {product['name']} is now {current_price} ‡¶ü‡¶æ‡¶ï‡¶æ"
                        send_notification(message)
                        print(message)

        # Save data with timestamp
        filename = f"price_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        self.scraper.save_data(current_data, filename)

    def extract_price(self, price_text):
        import re
        # Extract numeric price from text like "‡ß´‡ß¶,‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ"
        numbers = re.findall(r'[\d,]+', price_text)
        if numbers:
            return int(numbers[0].replace(',', ''))
        return 0

    def start_monitoring(self):
        # Schedule monitoring every hour
        schedule.every().hour.do(self.monitor_prices)

        print("üìä Price monitoring system started!")
        print("Press Ctrl+C to stop...")

        try:
            while True:
                schedule.run_pending()
                time.sleep(60)
        except KeyboardInterrupt:
            print("\nüõë Monitoring stopped!")

if __name__ == "__main__":
    monitor = PriceMonitor()
    monitor.start_monitoring()
```

### üì∞ **News Aggregator:**
```python
# news_aggregator.py
from playwright.sync_api import sync_playwright
import json
from datetime import datetime

class NewsAggregator:
    def __init__(self):
        self.news_sources = [
            {
                'name': 'Prothom Alo',
                'url': 'https://www.prothomalo.com',
                'title_selector': '.story-card__title',
                'link_selector': '.story-card__title a'
            },
            {
                'name': 'The Daily Star',
                'url': 'https://www.thedailystar.net',
                'title_selector': '.title',
                'link_selector': '.title a'
            }
        ]

    def scrape_news_source(self, source):
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()

            try:
                page.goto(source['url'])
                page.wait_for_load_state('networkidle')

                # Get all news titles and links
                titles = page.locator(source['title_selector']).all()
                links = page.locator(source['link_selector']).all()

                news_items = []
                for i, title in enumerate(titles[:10]):  # Top 10 news
                    try:
                        title_text = title.text_content().strip()
                        link_href = links[i].get_attribute('href') if i < len(links) else ''

                        # Make absolute URL
                        if link_href.startswith('/'):
                            link_href = source['url'] + link_href

                        news_items.append({
                            'title': title_text,
                            'link': link_href,
                            'source': source['name'],
                            'scraped_at': datetime.now().isoformat()
                        })
                    except Exception as e:
                        print(f"Error processing item {i}: {e}")
                        continue

                return news_items

            except Exception as e:
                print(f"Error scraping {source['name']}: {e}")
                return []

            finally:
                browser.close()

    def aggregate_all_news(self):
        all_news = []

        for source in self.news_sources:
            print(f"üì∞ Scraping {source['name']}...")
            news_items = self.scrape_news_source(source)
            all_news.extend(news_items)

            # Delay between sources
            import time
            time.sleep(2)

        # Save aggregated news
        filename = f"news_aggregated_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(f'data/raw/{filename}', 'w', encoding='utf-8') as f:
            json.dump(all_news, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ Total {len(all_news)} news items collected!")
        return all_news

# Usage
if __name__ == "__main__":
    aggregator = NewsAggregator()
    news = aggregator.aggregate_all_news()

    # Print top 5 news
    for i, item in enumerate(news[:5]):
        print(f"{i+1}. {item['title']} - {item['source']}")
```

### üè¢ **Job Listings Scraper:**
```python
# job_scraper.py
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime

class JobScraper:
    def __init__(self):
        self.base_url = "https://jobs.example.com"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

    def scrape_jobs(self, keyword="python developer", location="dhaka"):
        jobs = []
        page = 1

        while page <= 5:  # Scrape first 5 pages
            print(f"üîç Scraping page {page}...")

            params = {
                'q': keyword,
                'l': location,
                'p': page
            }

            response = requests.get(f"{self.base_url}/search",
                                  params=params, headers=self.headers)

            if response.status_code != 200:
                print(f"Error: {response.status_code}")
                break

            soup = BeautifulSoup(response.content, 'html.parser')
            job_cards = soup.find_all('div', class_='job-card')

            if not job_cards:
                print("No more jobs found!")
                break

            for card in job_cards:
                try:
                    job_data = {
                        'title': card.find('h3', class_='job-title').text.strip(),
                        'company': card.find('span', class_='company-name').text.strip(),
                        'location': card.find('span', class_='job-location').text.strip(),
                        'salary': self.extract_salary(card),
                        'posted_date': card.find('span', class_='posted-date').text.strip(),
                        'job_url': self.base_url + card.find('a')['href'],
                        'scraped_at': datetime.now().isoformat()
                    }
                    jobs.append(job_data)

                except Exception as e:
                    print(f"Error parsing job card: {e}")
                    continue

            page += 1
            import time
            time.sleep(2)  # Be respectful

        return jobs

    def extract_salary(self, card):
        salary_elem = card.find('span', class_='salary')
        if salary_elem:
            return salary_elem.text.strip()
        return "Not specified"

    def save_to_excel(self, jobs, filename):
        df = pd.DataFrame(jobs)

        # Data cleaning
        df['salary_numeric'] = df['salary'].str.extract('(\d+)').astype(float)

        # Save to Excel with multiple sheets
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='All Jobs', index=False)

            # High salary jobs (if salary info available)
            high_salary = df[df['salary_numeric'] > 50000]
            if not high_salary.empty:
                high_salary.to_excel(writer, sheet_name='High Salary', index=False)

            # Jobs by company
            company_summary = df.groupby('company').size().reset_index(name='job_count')
            company_summary.to_excel(writer, sheet_name='Company Summary', index=False)

        print(f"üìä Data saved to {filename}")

# Usage
if __name__ == "__main__":
    scraper = JobScraper()

    # Scrape Python developer jobs in Dhaka
    jobs = scraper.scrape_jobs("python developer", "dhaka")

    if jobs:
        filename = f"jobs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        scraper.save_to_excel(jobs, filename)

        print(f"‚úÖ Found {len(jobs)} jobs!")

        # Show top 5 jobs
        for i, job in enumerate(jobs[:5]):
            print(f"{i+1}. {job['title']} at {job['company']} - {job['location']}")
    else:
        print("‚ùå No jobs found!")
```

---

## üéì ‡¶∂‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶™‡¶• ‡¶ì ‡¶™‡¶∞‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶ß‡¶æ‡¶™

### üìö **‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ‡¶®‡¶¨‡¶ø‡¶∏ ‡¶•‡ßá‡¶ï‡ßá ‡¶è‡¶ï‡ßç‡¶∏‡¶™‡¶æ‡¶∞‡ßç‡¶ü ‡¶π‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶∞‡ßã‡¶°‡¶Æ‡ßç‡¶Ø‡¶æ‡¶™:**

#### ü•â **Level 1: Beginner (‡ßß-‡ß® ‡¶Æ‡¶æ‡¶∏)**
- [ ] HTML/CSS ‡¶¨‡ßá‡¶∏‡¶ø‡¶ï ‡¶∂‡¶ø‡¶ñ‡ßÅ‡¶®
- [ ] Python ‡¶¨‡ßá‡¶∏‡¶ø‡¶ï ‡¶∂‡¶ø‡¶ñ‡ßÅ‡¶®
- [ ] requests + BeautifulSoup ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶∏‡¶ø‡¶Æ‡ßç‡¶™‡¶≤ ‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶™‡¶ø‡¶Ç
- [ ] CSV ‡¶´‡¶æ‡¶á‡¶≤‡ßá ‡¶°‡ßá‡¶ü‡¶æ ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡¶æ
- [ ] DevTools ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ

#### ü•à **Level 2: Intermediate (‡ß®-‡ß™ ‡¶Æ‡¶æ‡¶∏)**
- [ ] Playwright/Selenium ‡¶∂‡¶ø‡¶ñ‡ßÅ‡¶®
- [ ] JavaScript rendering handle ‡¶ï‡¶∞‡¶æ
- [ ] Form submission ‡¶ì login
- [ ] Database ‡¶è ‡¶°‡ßá‡¶ü‡¶æ ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡¶æ
- [ ] Error handling ‡¶ì retry logic
- [ ] Basic data analysis with Pandas

#### ü•á **Level 3: Advanced (‡ß™-‡ß¨ ‡¶Æ‡¶æ‡¶∏)**
- [ ] Async/concurrent scraping
- [ ] API reverse engineering
- [ ] CAPTCHA solving
- [ ] Proxy rotation
- [ ] Distributed scraping
- [ ] Machine learning for data extraction

#### üèÜ **Level 4: Expert (‡ß¨+ ‡¶Æ‡¶æ‡¶∏)**
- [ ] Custom browser automation
- [ ] Anti-detection techniques
- [ ] Large-scale data pipelines
- [ ] Real-time monitoring systems
- [ ] Cloud deployment (AWS/GCP)
- [ ] Contributing to open-source projects

### üîó **‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞‡ßÄ ‡¶∞‡¶ø‡¶∏‡ßã‡¶∞‡ßç‡¶∏:**

#### üìñ **‡¶¨‡¶á:**
- "Web Scraping with Python" by Ryan Mitchell
- "Automate the Boring Stuff with Python" by Al Sweigart

#### üåê **‡¶ì‡¶Ø‡¶º‡ßá‡¶¨‡¶∏‡¶æ‡¶á‡¶ü:**
- [Scrapy Documentation](https://scrapy.org/)
- [Playwright Documentation](https://playwright.dev/)
- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/)

#### üé• **YouTube Channels:**
- Corey Schafer (Python tutorials)
- Tech With Tim (Web scraping tutorials)

#### üí¨ **‡¶ï‡¶Æ‡¶ø‡¶â‡¶®‡¶ø‡¶ü‡¶ø:**
- Reddit: r/webscraping
- Stack Overflow
- GitHub (open source projects)

---

## üéØ ‡¶∏‡¶Æ‡¶æ‡¶™‡¶®‡ßÄ

‡¶è‡¶á ‡¶ó‡¶æ‡¶á‡¶°‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶ì‡¶Ø‡¶º‡ßá‡¶¨ ‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶™‡¶ø‡¶Ç ‡¶è‡¶∞ A-Z ‡¶∏‡¶¨ ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶ï‡¶≠‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§ ‡¶Æ‡¶®‡ßá ‡¶∞‡¶æ‡¶ñ‡¶¨‡ßá‡¶®:

### ‚úÖ **‡¶Æ‡ßÇ‡¶≤ ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶ó‡ßÅ‡¶≤‡ßã:**
1. **‡¶ß‡ßà‡¶∞‡ßç‡¶Ø ‡¶∞‡¶æ‡¶ñ‡ßÅ‡¶®:** ‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶™‡¶ø‡¶Ç ‡¶∂‡ßá‡¶ñ‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ
2. **‡¶®‡ßà‡¶§‡¶ø‡¶ï‡¶§‡¶æ ‡¶Æ‡ßá‡¶®‡ßá ‡¶ö‡¶≤‡ßÅ‡¶®:** robots.txt ‡¶ì ToS respect ‡¶ï‡¶∞‡ßÅ‡¶®
3. **‡¶™‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶ï‡¶ü‡¶ø‡¶∏ ‡¶ï‡¶∞‡ßÅ‡¶®:** ‡¶õ‡ßã‡¶ü ‡¶™‡ßç‡¶∞‡¶ú‡ßá‡¶ï‡ßç‡¶ü ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡ßÅ‡¶®
4. **‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶•‡¶æ‡¶ï‡ßÅ‡¶®:** ‡¶ì‡¶Ø‡¶º‡ßá‡¶¨‡¶∏‡¶æ‡¶á‡¶ü ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶® ‡¶π‡¶Ø‡¶º, ‡¶ï‡ßã‡¶°‡¶ì ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®

### üöÄ **‡¶™‡¶∞‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶™‡¶¶‡¶ï‡ßç‡¶∑‡ßá‡¶™:**
1. ‡¶è‡¶á ‡¶ó‡¶æ‡¶á‡¶°‡ßá‡¶∞ ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£‡¶ó‡ßÅ‡¶≤‡ßã ‡¶ö‡¶æ‡¶≤‡¶ø‡¶Ø‡¶º‡ßá ‡¶¶‡ßá‡¶ñ‡ßÅ‡¶®
2. ‡¶®‡¶ø‡¶ú‡ßá‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶™‡ßç‡¶∞‡¶ú‡ßá‡¶ï‡ßç‡¶ü ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡ßÅ‡¶®
3. GitHub ‡¶è ‡¶ï‡ßã‡¶° ‡¶∂‡ßá‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
4. ‡¶ï‡¶Æ‡¶ø‡¶â‡¶®‡¶ø‡¶ü‡¶ø‡¶§‡ßá ‡¶Ø‡ßã‡¶ó ‡¶¶‡¶ø‡¶®

### üí° **‡¶Æ‡¶®‡ßá ‡¶∞‡¶æ‡¶ñ‡ßÅ‡¶®:**
> "The best way to learn web scraping is by doing it!"

**Happy Scraping! üï∑Ô∏èüáßüá©**

---

*‡¶è‡¶á ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü‡ßá‡¶∂‡¶® ‡¶®‡¶ø‡¶Ø‡¶º‡¶Æ‡¶ø‡¶§ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶π‡¶¨‡ßá‡•§ ‡¶®‡¶§‡ßÅ‡¶® ‡¶ü‡ßÅ‡¶≤‡¶∏ ‡¶ì ‡¶ü‡ßá‡¶ï‡¶®‡¶ø‡¶ï ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ ‡¶π‡¶¨‡ßá‡•§*
```
```
```
```
